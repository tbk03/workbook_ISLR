[["index.html", "Workbook: An Introduction to Statistical Learning Welcome", " Workbook: An Introduction to Statistical Learning Chris J. Martin 2021-06-03 Welcome "],["introduction.html", "Chapter 1 Introduction", " Chapter 1 Introduction There are no exercises in Chapter 1 (Introduction). "],["statistical-learning.html", "Chapter 2 Statistical Learning 2.1 Conceptual Exercises 2.2 Applied Exercises", " Chapter 2 Statistical Learning library(tidyverse) ## Warning: package &#39;tidyverse&#39; was built under R version 4.0.5 ## -- Attaching packages --------------------------------------- tidyverse 1.3.1 -- ## v ggplot2 3.3.3 v purrr 0.3.4 ## v tibble 3.1.0 v dplyr 1.0.5 ## v tidyr 1.1.3 v stringr 1.4.0 ## v readr 1.4.0 v forcats 0.5.1 ## Warning: package &#39;ggplot2&#39; was built under R version 4.0.4 ## Warning: package &#39;tibble&#39; was built under R version 4.0.5 ## Warning: package &#39;tidyr&#39; was built under R version 4.0.4 ## Warning: package &#39;dplyr&#39; was built under R version 4.0.4 ## Warning: package &#39;forcats&#39; was built under R version 4.0.4 ## -- Conflicts ------------------------------------------ tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() 2.1 Conceptual Exercises Question 1: Flexible and inflexible statistical learning methods. (a): If sample size (n) is very large and the number of predictors is small (p) I would expect a flexible statistical learning method to perform better than an inflexible method. This is where sample sizes are large one can assume that the variance will be a lesser contributor to the irreducible error than bias. And, the bias of the more flexible method will be lower than the bias of the more flexible method. (b): If there is a large number of predictors (p) and a small number of data points (n) I would expect a flexible statistical learning method to perform worse than an inflexible method. This is because a flexible method would be likely to have high variance (i.e. the component of the reducible error associated with the differences between model prediction over different training sets). Hence, the more flexible method would be more prone to overfitting the small number of training data points (than the inflexible method) and in turn be likely to return worse results. This is of course a generalisation and the performance of flexible vs. inflexible methods varies from dataset to dataset. (c): If the relationship between the response variable and the predictor variables is highly non linear I would expect a flexible statistical learning method to perform better than an inflexible method. This is because a relatively inflexible methods (such as linear regression) tend to have assumptions around linearity embedded within them, whereas more flexible methods do not (e.g. splines). (d): If the variance of the error terms is extremely high I would expect a flexible statistical learning method to perform worse than an inflexible method. This is because with a flexible model there would a tendency to overfit the large amount noise (i.e. very high variance in error terms) in the data. Whereas, the less flexible method would be less sensitive to the noise and in many cases could well do a better job of extracting the signal (i.e. relationship) between the predictors and the response variable. Question 2: Regression or classification problems. (a): Problem type: regression (salary is a continuous variable); n = 500; and, p = 3. (b): Problem type: classification (success or failure is a binary variable); n = 20; and, p = 13. (c): Problem type - regression (% change is a continuous variable); n = 52; and, p = 3. Question 3: Bias-variance decomposition (a): sketched in paper notebook. (b): describing the shape of typical error curves Bias (squared) - reduces as the flexibility of method increases because the bias represents the errors by approximating and simplifying a real world system. Less flexible methods apply more simplifications than flexible methods and hence have greater bias. Variance - increase as the flexibility of a method increases because the variance represents the error arising from sample process. More flexible methods are more closely fitted to the specific features of the data than less flexible models. So, overfitting of the current sample increases of model flexibility, driving increases in variance errors. The Training error - decrease as the flexibility of a method increases (following the bias curve plus the irreducible error). This is because variance errors are not evident in fitting to the training set, rather overfitting actually reduce training errors (giving an overly optimistic perspective on the likely performance of a method on new data). The test error - decreases as method flexibility increases but then increases again. This is because although the test error decrease as the bias component of the errors decreases, at some point increases in the variance component of the error drives increases in the test error. In other words there is a point at which the advantages of increase flexibility (in terms of reducing the bias component) are outweigh by the disadvantages (in terms of overfitting as the variance component increases. The irreducible error - is constant and by definition is the component of the error that is unaffected by method choice and parameterisation. Question 4: Examples problems which are well suited to statistical learning methods. Classification problems: Identifying if an email is or is not spam is a problem where predictions are required. The response variable would be flag indicating if an email is spam or not (in the training set this might have been set by manually reviewing the email). The predictor variables would be characteristics of the email for example the use of certain words that are prevalent in spam. Explaining difference in the proportion of professors who have tenured. The response variable would be a variable indicating if the professor in question has tenure. The explanatory variables could be characteristic including the gender of the professor, if they have a minority ethnic background etc. Predicting if an athlete will win a medal (or not) at the Olympics in a specific event. Predictor variables here could include for example performance at past Olympics, personal bests etc. Regression problems: Estimating the amount of plastic waste produced by a country. Here predictor variables could include GDP, population, quantitative measure of the maturity of the waste disposal system etc. Predicting if a given individual will make use of green spaces in their local area. Here predictor variables might include self-reported health status, age etc. Explaining the Green House Gas emission reductions (or increases) delivered by a country. Here predictor values might include GDP per capita, a categorical variable indicating how supportive a countries is at COP meetings, the political leanings of the governing party (on a left-right axis) etc. Clustering problems: Identifying if there are groups of neighborhoods within a city facing similar problems with citizens accessing green space. Here features for clustering might include the proportion of the population within easy walking distance of public green space, the amount of public green space in the neighborhood, the proportion of the population experiencing disabilities. Identifying if there are groups of customer (of a given business) with similar purchasing habit. Here feature for clustering might total spend per month with the business, the frequency of purchases etc. Identify if groups of people with similar attitudes to car sharing can be identified from survey responses. Here features for clustering might include Likhert scales responses asking about attitudes to car sharing. Question 5: advantages and disadvantages of a very flexible method (relative to an inflexible method). Advantages: ability to capture non-linear relationships between variables; ability to pick up relatively subtle associations between variables; can deliver more accurate prediction by modeling more of the complexity of real world systems. Disadvantages: prone to overfitting; large numbers of data points are required; computational intensive; can be difficult to interpret the model, and hence not so useful for inference. Question 6: parametric and non-parametric methods A parametric model makes as assumptions about the functional form (i.e. shape) of f. Where f is the function mapping the values of predictor/s to the values of the response variable. Based on these assumptions the general form of the model can be mathematical expressed including the predictor variables and associated co-efficients. Once a parametric model has been selected it can be fit to the data, this reduces to a problem of estimating the coefficients in the mathematical model. The primary advantage of this approach is its simplicity (estimating a set of parameters is much simpler than estimating f itself). The disadvantages are that: it can be difficult to know which model (i.e. shape) to select and picking the wrong model results in poor results; and, that flexible models are prone to overfitting. A non-parametric model does not make any assumptions about the functional form of f. Rather such models attempt to estimate f by fitting the points as closely as possible, subject to specified constraints on how wiggly or smooth a line should result. The main advantage of non-parametric models is that they can fit a wide variety of shapes for f and the analyst does not need to make any a priori guesses about what the associated between predictor and response variables are. However, this advantage comes at the cost of complexity. As the problems of estimating f is not reduced to one of estimating parameters, very large numbers of data points are needed. Also, given non-parametric models are not constrained by assumption about the shape of f then care is needed to avoid overfitting (i.e. an appropriate smoothness constraint must be defined by the analyst). Question 7: K-nearest neighbors (a): the euclidean distance between each data point and a test point (x1 = x2 = x3 = 0) is shown below. library(tidyverse) data &lt;- read_csv(&quot;2_7.csv&quot;) %&gt;% mutate(dist_from_test_point = sqrt(x1^2 + x2^2 + x3^2)) data %&gt;% knitr::kable() obs x1 x2 x3 y dist_from_test_point 1 0 3 0 Red 3.000000 2 2 0 0 Red 2.000000 3 0 1 3 Red 3.162278 4 0 1 2 Green 2.236068 5 -1 0 1 Green 1.414214 6 1 1 1 Red 1.732051 data %&gt;% select(obs, dist_from_test_point) %&gt;% knitr::kable() obs dist_from_test_point 1 3.000000 2 2.000000 3 3.162278 4 2.236068 5 1.414214 6 1.732051 (b): Using the k-nearest neighbor method with k = 1 for the test point (x1 = x2 = x3 = 0) the prediction would be the Y value of the nearest point. The nearest point is 5 so the prediction would be Green. (c): If k =3, k-nearest neighbor would look at the three nearest points (y2 = Red, y5 = Green and y6 = Red). The predicted y value would be Red as the majority of the k nearest points have an observed y values of Red. 2.2 Applied Exercises Question 8: The Colleges Dataset (a): first I read in the data for the package. library(ISLR) library(pander) college &lt;- as_tibble(College) (b): There college names are already stored as row names pander(head(college)) Table continues below Private Apps Accept Enroll Top10perc Top25perc F.Undergrad Yes 1660 1232 721 23 52 2885 Yes 2186 1924 512 16 29 2683 Yes 1428 1097 336 22 50 1036 Yes 417 349 137 60 89 510 Yes 193 146 55 16 44 249 Yes 587 479 158 38 62 678 Table continues below P.Undergrad Outstate Room.Board Books Personal PhD Terminal 537 7440 3300 450 2200 70 78 1227 12280 6450 750 1500 29 30 99 11250 3750 400 1165 53 66 63 12960 5450 450 875 92 97 869 7560 4120 800 1500 76 72 41 13500 3335 500 675 67 73 S.F.Ratio perc.alumni Expend Grad.Rate 18.1 12 7041 60 12.2 16 10527 56 12.9 30 8735 54 7.7 37 19016 59 11.9 2 10922 15 9.4 11 9727 55 (c): First I produce the summary statistics. college %&gt;% skimr::skim() Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts Private 0 1 FALSE 2 Yes: 565, No: 212 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist Apps 0 1 3001.64 3870.20 81.0 776.0 1558.0 3624.0 48094.0  Accept 0 1 2018.80 2451.11 72.0 604.0 1110.0 2424.0 26330.0  Enroll 0 1 779.97 929.18 35.0 242.0 434.0 902.0 6392.0  Top10perc 0 1 27.56 17.64 1.0 15.0 23.0 35.0 96.0  Top25perc 0 1 55.80 19.80 9.0 41.0 54.0 69.0 100.0  F.Undergrad 0 1 3699.91 4850.42 139.0 992.0 1707.0 4005.0 31643.0  P.Undergrad 0 1 855.30 1522.43 1.0 95.0 353.0 967.0 21836.0  Outstate 0 1 10440.67 4023.02 2340.0 7320.0 9990.0 12925.0 21700.0  Room.Board 0 1 4357.53 1096.70 1780.0 3597.0 4200.0 5050.0 8124.0  Books 0 1 549.38 165.11 96.0 470.0 500.0 600.0 2340.0  Personal 0 1 1340.64 677.07 250.0 850.0 1200.0 1700.0 6800.0  PhD 0 1 72.66 16.33 8.0 62.0 75.0 85.0 103.0  Terminal 0 1 79.70 14.72 24.0 71.0 82.0 92.0 100.0  S.F.Ratio 0 1 14.09 3.96 2.5 11.5 13.6 16.5 39.8  perc.alumni 0 1 22.74 12.39 0.0 13.0 21.0 31.0 64.0  Expend 0 1 9660.17 5221.77 3186.0 6751.0 8377.0 10830.0 56233.0  Grad.Rate 0 1 65.46 17.18 10.0 53.0 65.0 78.0 118.0  Then produce a scatter plot matrix for the first 10 variables. (c): next is to plot side by side boxplots of outstate vs private college %&gt;% ggplot(aes(x = Outstate, y = Private)) + geom_boxplot() + coord_flip() + labs(x = &quot;Private university?&quot;, y = &quot;Number of out of state students&quot;, title = &quot;Private universities tend to have more out of state students&quot;) (iv): next is looking at universities with high proportions of students from elite high schools. college %&gt;% mutate(elite = if_else(Top10perc &gt; 50, TRUE, FALSE)) %&gt;% group_by(elite) %&gt;% summarise(n = n()) ## # A tibble: 2 x 2 ## elite n ## &lt;lgl&gt; &lt;int&gt; ## 1 FALSE 699 ## 2 TRUE 78 (v): next looking at producing some histograms for a couple of continuous variables of interest (full and part time undergraduates) library(patchwork) p &lt;- college %&gt;% select(F.Undergrad, P.Undergrad) %&gt;% mutate(id = row_number(), .before = F.Undergrad) %&gt;% pivot_longer(cols = c(F.Undergrad, P.Undergrad), names_to = &quot;student_status&quot;, values_to = &quot;number_of_students&quot;) %&gt;% ggplot(aes(number_of_students)) + geom_histogram(aes(fill = student_status), bins = 100) + facet_wrap(~student_status) + guides(fill = FALSE) + labs(x = &quot;Number of students attending&quot;, y = &quot;Number of universities&quot;) p_zoom &lt;- p + coord_cartesian(xlim = c(0,2500)) + labs(subtitle = &quot;Zooming on smaller cohort sizes&quot;) p / p_zoom + plot_annotation(title = &quot;Differing frequencies of full and part-time students cohort sizes&quot;) (vi): Continuing the exploratory data analysis, my key findings were as follows: Private universities appear to have higher graduation rates than public universities. college %&gt;% # remove an outlier with impossible graduation rate filter(Grad.Rate &lt;= 100) %&gt;% # produce plot base ggplot(aes(Private, Grad.Rate)) + # add data points as background layer geom_jitter(alpha = 0.5, width = 0.2, colour = &quot;lightblue&quot;) + geom_boxplot(fill = NA, outlier.shape = NA, size = 1) + # add labels labs(x = &quot;Private university?&quot;, y = &quot;Graduation rate&quot;, title = &quot;Private universities appear to have higher graduation rates\\n&quot;) More selective universities appear to have higher graduation rates. college %&gt;% # remove an outlier with impossible graduation rate filter(Grad.Rate &lt;= 100) %&gt;% # create plot base ggplot(aes(cut_width(Accept / Apps * 100, width = 20, boundary = 0), Grad.Rate)) + # add data points as a background layer geom_jitter(alpha = 0.5, colour = &quot;lightblue&quot;, width = 0.2)+ # add boxplots geom_boxplot(width = 0.2, alpha = 0) + # format scales for readability scale_x_discrete(labels = c(&quot;0-19%&quot;, &quot;20-39%&quot;, &quot;40-59%&quot;, &quot;60-79%&quot;, &quot;80-100%&quot;)) + scale_y_continuous(labels = scales::percent_format(scale = 1)) + # add labels labs(x = &quot;\\nPercentage of applications accepted&quot;, y = &quot;Percentage of students graduating\\n&quot;, title = &quot;More selective universities appear to have higher graduation rates\\n&quot;) Question 9: The Auto dataset (a): Quantitative predictors: mpg, cylinders, displacement, horsepower, weight, acceleration, year and origin. Qualitative predictor: name. (b): The range for each quantiative variable is shown in the skim dataframe below (p0 corresponds to the minimum and p100 corresponds to the maximum). (c): The mean and standard deviations for each quantiative variable is shown in the skim dataframe below. skimr::skim(Auto) %&gt;% select(-n_missing, -complete_rate) Variable type: factor skim_variable ordered n_unique top_counts name FALSE 301 amc: 5, for: 5, toy: 5, amc: 4 Variable type: numeric skim_variable mean sd p0 p25 p50 p75 p100 hist mpg 23.45 7.81 9 17.00 22.75 29.00 46.6  cylinders 5.47 1.71 3 4.00 4.00 8.00 8.0  displacement 194.41 104.64 68 105.00 151.00 275.75 455.0  horsepower 104.47 38.49 46 75.00 93.50 126.00 230.0  weight 2977.58 849.40 1613 2225.25 2803.50 3614.75 5140.0  acceleration 15.54 2.76 8 13.78 15.50 17.02 24.8  year 75.98 3.68 70 73.00 76.00 79.00 82.0  origin 1.58 0.81 1 1.00 1.00 2.00 3.0  (d): The means and standard deviations for the quantitative variables for the dataset (minus the 10th and 85th rows) are shown in the skim dataframe below. autos_omit &lt;- slice(Auto, -10, -85) skimr::skim(autos_omit) %&gt;% select(-n_missing, -complete_rate) Variable type: factor skim_variable ordered n_unique top_counts name FALSE 299 amc: 5, for: 5, toy: 5, amc: 4 Variable type: numeric skim_variable mean sd p0 p25 p50 p75 p100 hist mpg 23.49 7.80 9 17.50 23.0 29.00 46.6  cylinders 5.46 1.70 3 4.00 4.0 8.00 8.0  displacement 193.51 104.14 68 105.00 148.5 262.00 455.0  horsepower 104.07 38.18 46 75.00 92.5 125.00 230.0  weight 2972.47 848.51 1613 2223.75 2797.5 3608.00 5140.0  acceleration 15.57 2.74 8 13.83 15.5 17.08 24.8  year 76.00 3.68 70 73.00 76.0 79.00 82.0  origin 1.58 0.81 1 1.00 1.0 2.00 3.0  (e and f): The correlation plot below shows a high degree of correlation between four predictor variables (cylinders, displacement, horsepower and weight). There is also a high degree of correlation between these three predictors and the response variable (mpg). GGally::ggcorr(Auto, label = TRUE) Scatter plots can help use look at these four predictor variables in more detail. ggplot(Auto, aes(factor(cylinders), mpg)) + geom_jitter(colour = &quot;lightblue&quot;, width = 0.2) + geom_boxplot(width = 0.3, alpha = 0) + labs(x = &quot;\\nNumber of Cylinders&quot;, y = &quot;Fuel economy (mpg)\\n&quot;, title = &quot;There is a negative association between the number of cylinders\\nand fuel economy\\n&quot;) ggplot(Auto, aes(displacement, mpg)) + geom_point(alpha = 0.5) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + scale_x_log10(breaks = c(0, 10, 100, 200, 400)) + expand_limits(x = 0, y = 0) + labs(x = &quot;\\nEngine displacement (on a log transformed scale)\\n&quot;, y = &quot;Fuel economy (mpg)\\n&quot;, title = &quot;There is a negative association between the\\nlogarithm of engine displacement and fuel economy\\n&quot;) ggplot(Auto, aes(horsepower, mpg)) + geom_point(alpha = 0.5) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + scale_x_log10() + expand_limits(x = 0, y = 0) + labs(x = &quot;Horsepower (on a log scale)&quot;, y = &quot;Fuel Economy (mpg)&quot;, title = &quot;There is a negative association between the\\nlogarithm of horsepower and fuel economy\\n&quot;) ggplot(Auto, aes(weight, mpg)) + geom_point(alpha = 0.5) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + scale_x_log10() + labs(x = &quot;Weight in kgs (on a log scale)&quot;, y = &quot;Fuel economy (mpg)&quot;, title = &quot;There is a negative association between the\\nlogarithm of weight and fuel economy\\n&quot;) Each of the four variables plotted above could be useful predictors of fuel economy. Additionally, the correlation plot also identified two other variables with reasonably strong correlations with fuel economy: year and origin. If I was going to proceed to develop a model using this data, it would be worth looking at year and origin in more detail. Question 8: The Boston Housing dataset. "],["linear-regression.html", "Chapter 3 Linear Regression 3.1 Lab 3.2 Conceptual questions 3.3 Applied questions", " Chapter 3 Linear Regression library(MASS) # For Boston data set ## Warning: package &#39;MASS&#39; was built under R version 4.0.4 library(tidymodels) ## -- Attaching packages -------------------------------------- tidymodels 0.1.2 -- ## v broom 0.7.6 v recipes 0.1.15 ## v dials 0.0.9 v rsample 0.0.9 ## v dplyr 1.0.5 v tibble 3.1.0 ## v ggplot2 3.3.3 v tidyr 1.1.3 ## v infer 0.5.4 v tune 0.1.3 ## v modeldata 0.1.0 v workflows 0.2.2 ## v parsnip 0.1.5 v yardstick 0.0.7 ## v purrr 0.3.4 ## Warning: package &#39;broom&#39; was built under R version 4.0.5 ## Warning: package &#39;dplyr&#39; was built under R version 4.0.4 ## Warning: package &#39;ggplot2&#39; was built under R version 4.0.4 ## Warning: package &#39;infer&#39; was built under R version 4.0.4 ## Warning: package &#39;parsnip&#39; was built under R version 4.0.4 ## Warning: package &#39;rsample&#39; was built under R version 4.0.4 ## Warning: package &#39;tibble&#39; was built under R version 4.0.5 ## Warning: package &#39;tidyr&#39; was built under R version 4.0.4 ## Warning: package &#39;tune&#39; was built under R version 4.0.4 ## Warning: package &#39;workflows&#39; was built under R version 4.0.4 ## -- Conflicts ----------------------------------------- tidymodels_conflicts() -- ## x purrr::discard() masks scales::discard() ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() ## x dplyr::select() masks MASS::select() ## x recipes::step() masks stats::step() library(ISLR) ## Warning: package &#39;ISLR&#39; was built under R version 4.0.5 library(tidyverse) ## Warning: package &#39;tidyverse&#39; was built under R version 4.0.5 ## -- Attaching packages --------------------------------------- tidyverse 1.3.1 -- ## v readr 1.4.0 v forcats 0.5.1 ## v stringr 1.4.0 ## Warning: package &#39;forcats&#39; was built under R version 4.0.4 ## -- Conflicts ------------------------------------------ tidyverse_conflicts() -- ## x readr::col_factor() masks scales::col_factor() ## x purrr::discard() masks scales::discard() ## x dplyr::filter() masks stats::filter() ## x stringr::fixed() masks recipes::fixed() ## x dplyr::lag() masks stats::lag() ## x dplyr::select() masks MASS::select() ## x readr::spec() masks yardstick::spec() 3.1 Lab Based on ISLR tidymodels Labs A simple linear regression model # specify the model lm_spec &lt;- parsnip::linear_reg() %&gt;% parsnip::set_mode(&quot;regression&quot;) %&gt;% parsnip::set_engine(&quot;stan&quot;) lm_spec ## Linear Regression Model Specification (regression) ## ## Computational engine: stan # fit the model lm_fit &lt;- lm_spec %&gt;% parsnip::fit(medv ~ lstat, data = Boston) lm_fit ## parsnip model object ## ## Fit time: 620ms ## stan_glm ## family: gaussian [identity] ## formula: medv ~ lstat ## observations: 506 ## predictors: 2 ## ------ ## Median MAD_SD ## (Intercept) 34.6 0.6 ## lstat -0.9 0.0 ## ## Auxiliary parameter(s): ## Median MAD_SD ## sigma 6.2 0.2 ## ## ------ ## * For help interpreting the printed output see ?print.stanreg ## * For info on the priors used see ?prior_summary.stanreg # examine the fit lm_fit$fit ## stan_glm ## family: gaussian [identity] ## formula: medv ~ lstat ## observations: 506 ## predictors: 2 ## ------ ## Median MAD_SD ## (Intercept) 34.6 0.6 ## lstat -0.9 0.0 ## ## Auxiliary parameter(s): ## Median MAD_SD ## sigma 6.2 0.2 ## ## ------ ## * For help interpreting the printed output see ?print.stanreg ## * For info on the priors used see ?prior_summary.stanreg # in more details lm_fit %&gt;% purrr::pluck(&quot;fit&quot;) %&gt;% summary() ## ## Model Info: ## function: stan_glm ## family: gaussian [identity] ## formula: medv ~ lstat ## algorithm: sampling ## sample: 4000 (posterior sample size) ## priors: see help(&#39;prior_summary&#39;) ## observations: 506 ## predictors: 2 ## ## Estimates: ## mean sd 10% 50% 90% ## (Intercept) 34.6 0.6 33.8 34.6 35.3 ## lstat -0.9 0.0 -1.0 -0.9 -0.9 ## sigma 6.2 0.2 6.0 6.2 6.5 ## ## Fit Diagnostics: ## mean sd 10% 50% 90% ## mean_PPD 22.5 0.4 22.0 22.6 23.1 ## ## The mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help(&#39;summary.stanreg&#39;)). ## ## MCMC diagnostics ## mcse Rhat n_eff ## (Intercept) 0.0 1.0 3655 ## lstat 0.0 1.0 3698 ## sigma 0.0 1.0 3294 ## mean_PPD 0.0 1.0 3462 ## log-posterior 0.0 1.0 1658 ## ## For each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1). # get tidy output from model fit broom.mixed::tidy(lm_fit) ## # A tibble: 2 x 3 ## term estimate std.error ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 34.6 0.560 ## 2 lstat -0.949 0.0381 broom.mixed::glance(lm_fit) ## # A tibble: 1 x 4 ## algorithm pss nobs sigma ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 sampling 4000 506 6.22 broom.mixed::tidy(lm_fit$fit) ## # A tibble: 2 x 3 ## term estimate std.error ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 34.6 0.560 ## 2 lstat -0.949 0.0381 broom.mixed::glance(lm_fit$fit) ## # A tibble: 1 x 4 ## algorithm pss nobs sigma ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 sampling 4000 506 6.22 # predict medv for the training set stats::predict(lm_fit, new_data = Boston, type = &quot;conf_int&quot;) %&gt;% dplyr::bind_cols(Boston) %&gt;% dplyr::select(medv, tidyr::starts_with(&quot;.pred&quot;)) ## # A tibble: 506 x 3 ## medv .pred_lower .pred_upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 24 29.0 30.6 ## 2 21.6 25.3 26.5 ## 3 34.7 29.9 31.6 ## 4 33.4 30.9 32.7 ## 5 36.2 28.7 30.3 ## 6 28.7 28.8 30.4 ## 7 22.9 22.2 23.3 ## 8 27.1 15.6 17.1 ## 9 16.5 4.74 7.51 ## 10 18.9 17.7 19.0 ## # ... with 496 more rows # predict and compare to actual is less code broom::augment(lm_fit, new_data = Boston) %&gt;% dplyr::select(medv, `.pred`) ## medv .pred ## 1 24.0 29.8276951 ## 2 21.6 25.8793218 ## 3 34.7 30.7293669 ## 4 33.4 31.7639166 ## 5 36.2 29.4955002 ## 6 28.7 29.6093956 ## 7 22.9 22.7566900 ## 8 27.1 16.3785485 ## 9 16.5 6.1469465 ## 10 18.9 18.3242613 ## 11 15.0 15.1446818 ## 12 18.9 21.9594223 ## 13 21.7 19.6435495 ## 14 20.4 26.7145546 ## 15 18.2 24.8162982 ## 16 19.9 26.5152376 ## 17 23.1 28.3090899 ## 18 17.5 20.6306428 ## 19 20.2 23.4590448 ## 20 18.2 23.8481874 ## 21 13.6 14.6036787 ## 22 19.6 21.4279105 ## 23 15.2 16.7866736 ## 24 14.5 15.6856849 ## 25 15.6 19.0835638 ## 26 13.9 18.8842469 ## 27 16.6 20.4977649 ## 28 14.8 18.1534182 ## 29 18.4 22.4055125 ## 30 21.0 23.1837977 ## 31 12.7 13.1040562 ## 32 14.5 22.1777218 ## 33 13.2 8.2540111 ## 34 13.1 17.1378510 ## 35 13.5 15.2490859 ## 36 18.9 25.3667925 ## 37 20.0 23.7248007 ## 38 21.0 26.2304992 ## 39 24.7 24.9396848 ## 40 30.8 30.4541197 ## 41 34.9 32.6750797 ## 42 26.6 29.9605730 ## 43 25.3 29.0399187 ## 44 24.7 27.4928397 ## 45 21.2 25.4901792 ## 46 19.3 24.8637546 ## 47 20.0 21.1241895 ## 48 16.6 16.7107433 ## 49 14.4 5.3117137 ## 50 19.4 19.1784767 ## 51 19.7 21.7885792 ## 52 20.5 25.6040746 ## 53 25.0 29.5429566 ## 54 23.4 26.5532028 ## 55 18.9 20.5072561 ## 56 35.4 29.9890469 ## 57 24.7 29.0778838 ## 58 31.6 30.8052971 ## 59 23.3 28.0433340 ## 60 19.6 25.8033915 ## 61 18.7 22.0733177 ## 62 16.0 20.8489423 ## 63 22.2 28.1667207 ## 64 25.0 25.5376356 ## 65 33.0 26.9138715 ## 66 23.5 30.1219248 ## 67 19.4 24.8352807 ## 68 22.0 26.8664151 ## 69 17.4 22.1302654 ## 70 20.9 26.2115166 ## 71 24.2 28.1762120 ## 72 21.7 25.1769669 ## 73 22.8 29.3151658 ## 74 23.4 27.3979269 ## 75 24.1 28.1192643 ## 76 21.4 26.0691474 ## 77 20.0 23.1932889 ## 78 20.8 24.8068069 ## 79 21.2 22.8421115 ## 80 20.3 25.9172869 ## 81 28.0 29.5334653 ## 82 23.9 27.7016479 ## 83 24.8 28.1762120 ## 84 22.9 27.4264007 ## 85 23.9 25.4237402 ## 86 26.6 28.3565464 ## 87 22.5 22.3485648 ## 88 22.2 26.5437115 ## 89 23.6 29.3341484 ## 90 28.7 29.1443228 ## 91 22.6 26.1925341 ## 92 22.0 26.7715023 ## 93 22.9 26.8094674 ## 94 25.0 28.6602674 ## 95 20.6 24.5030859 ## 96 28.4 28.2426510 ## 97 21.4 23.7912397 ## 98 38.7 30.5585238 ## 99 43.8 31.1659658 ## 100 33.2 28.6792499 ## 101 27.5 25.6135659 ## 102 26.5 27.2745402 ## 103 18.6 24.4651207 ## 104 19.3 21.7980705 ## 105 20.1 22.8516028 ## 106 19.5 18.9222120 ## 107 19.5 16.8436213 ## 108 20.4 21.1811372 ## 109 19.8 22.9085505 ## 110 19.4 19.7954100 ## 111 21.7 22.2156869 ## 112 22.8 24.9112110 ## 113 18.8 19.1689854 ## 114 18.7 18.3337526 ## 115 18.5 24.6359638 ## 116 18.3 19.5960931 ## 117 21.2 23.1268500 ## 118 19.2 24.7783330 ## 119 20.4 19.9662531 ## 120 19.3 21.6367187 ## 121 22.0 20.9153813 ## 122 20.3 21.0102941 ## 123 20.5 17.5364849 ## 124 17.3 10.4370059 ## 125 18.8 17.8686797 ## 126 21.4 20.4977649 ## 127 15.7 8.6811188 ## 128 16.2 18.2388397 ## 129 18.0 19.9472705 ## 130 14.3 17.1473423 ## 131 19.2 22.5953382 ## 132 19.6 22.9180418 ## 133 23.0 24.0000479 ## 134 18.4 20.2889567 ## 135 15.6 18.1249444 ## 136 18.1 18.4571392 ## 137 17.4 18.5140869 ## 138 17.1 20.7065731 ## 139 13.3 14.3189403 ## 140 17.8 17.0334469 ## 141 14.0 11.6234162 ## 142 14.4 1.8948521 ## 143 13.4 9.0987352 ## 144 15.6 9.4783865 ## 145 11.8 6.7543885 ## 146 13.8 8.1685895 ## 147 15.6 18.7513690 ## 148 14.6 6.5265978 ## 149 17.8 7.6750429 ## 150 15.4 14.1955536 ## 151 21.5 21.1716459 ## 152 19.6 21.9499310 ## 153 15.3 23.0509197 ## 154 19.4 19.5676192 ## 155 17.0 20.2035351 ## 156 15.6 20.2984479 ## 157 13.1 19.2354244 ## 158 41.3 30.1978551 ## 159 24.3 28.4514592 ## 160 23.3 27.5402961 ## 161 27.0 29.3341484 ## 162 50.0 32.9123617 ## 163 50.0 32.7320274 ## 164 50.0 31.4032479 ## 165 22.7 23.5065013 ## 166 25.0 25.2434059 ## 167 50.0 31.0425792 ## 168 23.8 23.0319372 ## 169 23.8 24.0190305 ## 170 22.3 23.8102223 ## 171 17.4 20.8584336 ## 172 19.1 23.1363413 ## 173 23.1 20.6116602 ## 174 23.6 25.9742346 ## 175 22.6 25.4047577 ## 176 29.4 29.4955002 ## 177 23.2 24.9586674 ## 178 24.6 28.5843371 ## 179 29.9 27.9863864 ## 180 37.2 29.7707474 ## 181 39.8 27.3789443 ## 182 36.2 25.5850920 ## 183 37.9 29.9795556 ## 184 32.5 29.1633053 ## 185 26.4 21.2855413 ## 186 29.6 22.0733177 ## 187 50.0 30.3307330 ## 188 32.0 28.2141771 ## 189 29.8 30.2263289 ## 190 34.9 29.4385525 ## 191 37.0 29.7137997 ## 192 30.5 30.1029422 ## 193 36.4 31.8303556 ## 194 31.1 29.7802387 ## 195 29.1 30.3971720 ## 196 50.0 31.7354427 ## 197 33.3 30.6819104 ## 198 30.3 26.3823597 ## 199 34.6 28.2711248 ## 200 34.9 30.2263289 ## 201 32.9 30.3307330 ## 202 24.1 27.5023310 ## 203 42.3 31.6025648 ## 204 48.5 30.9381751 ## 205 50.0 31.8208643 ## 206 22.6 24.2373300 ## 207 24.4 24.1424171 ## 208 22.5 17.4130982 ## 209 24.4 20.6401341 ## 210 20.0 12.6389834 ## 211 21.7 18.1629095 ## 212 19.3 11.7942593 ## 213 22.4 19.3398285 ## 214 28.1 25.6515310 ## 215 23.7 6.5076152 ## 216 25.0 25.5661094 ## 217 23.3 21.7316315 ## 218 28.7 25.3573012 ## 219 21.5 17.5459762 ## 220 23.0 24.5885074 ## 221 26.7 25.3383187 ## 222 21.7 14.1860623 ## 223 27.5 25.1295105 ## 224 30.1 27.3409792 ## 225 44.8 30.6249628 ## 226 50.0 30.1598899 ## 227 37.6 31.5835822 ## 228 31.6 28.5178981 ## 229 46.7 30.8337710 ## 230 31.5 30.9856315 ## 231 24.3 23.4970100 ## 232 31.7 29.5714304 ## 233 41.7 32.2100068 ## 234 48.3 30.8052971 ## 235 29.0 26.9138715 ## 236 24.0 24.2278387 ## 237 25.1 25.4996705 ## 238 31.5 30.0649771 ## 239 23.7 28.5178981 ## 240 23.3 27.5592787 ## 241 22.0 23.7532746 ## 242 20.1 22.7851638 ## 243 22.2 23.9051351 ## 244 23.7 29.6283781 ## 245 17.6 22.6902510 ## 246 18.5 17.0334469 ## 247 24.3 25.8603392 ## 248 20.5 24.9207023 ## 249 24.5 25.5186530 ## 250 26.2 28.3280725 ## 251 24.4 28.9544971 ## 252 24.8 31.1469833 ## 253 29.6 31.2039310 ## 254 42.8 31.1944397 ## 255 21.9 28.3185812 ## 256 20.9 25.7749176 ## 257 44.0 31.6025648 ## 258 50.0 29.6948171 ## 259 36.0 27.1606448 ## 260 30.1 28.0053689 ## 261 33.8 25.4522141 ## 262 43.1 27.6636828 ## 263 48.8 28.9450058 ## 264 31.0 23.8766612 ## 265 36.5 26.8664151 ## 266 22.8 24.6359638 ## 267 30.7 20.5167474 ## 268 50.0 27.4928397 ## 269 43.5 31.5551084 ## 270 20.7 21.5987536 ## 271 21.1 22.2156869 ## 272 25.2 28.2995987 ## 273 24.4 27.2175925 ## 274 35.2 28.3090899 ## 275 32.4 31.2039310 ## 276 32.0 31.7259515 ## 277 33.2 28.8121279 ## 278 33.1 30.6059802 ## 279 29.1 27.7301217 ## 280 35.1 29.9510817 ## 281 45.4 30.9856315 ## 282 35.4 30.1978551 ## 283 46.0 31.6974776 ## 284 50.0 31.5551084 ## 285 32.2 27.1036971 ## 286 22.0 26.7430284 ## 287 20.1 22.2821259 ## 288 23.2 27.7775782 ## 289 22.3 27.3409792 ## 290 24.8 25.5281443 ## 291 28.5 31.3937566 ## 292 37.3 31.1754571 ## 293 27.9 30.0934510 ## 294 23.9 26.4108335 ## 295 21.7 24.6834202 ## 296 28.6 28.6033197 ## 297 27.1 27.5402961 ## 298 20.3 19.5201628 ## 299 22.5 29.8371863 ## 300 29.0 30.0554858 ## 301 24.8 28.7931453 ## 302 22.0 25.5376356 ## 303 26.4 26.3254120 ## 304 33.1 29.9415904 ## 305 36.1 27.9768951 ## 306 28.4 26.0786387 ## 307 33.4 28.4134940 ## 308 28.2 27.4074182 ## 309 22.8 30.2453115 ## 310 20.3 25.0915453 ## 311 16.1 22.5573731 ## 312 22.1 28.8785669 ## 313 19.4 23.4305710 ## 314 21.6 27.0562407 ## 315 23.8 25.7464438 ## 316 16.2 23.6393792 ## 317 17.8 17.1568336 ## 318 19.8 19.4252500 ## 319 23.1 24.7213853 ## 320 21.0 22.4719515 ## 321 23.8 27.7206305 ## 322 23.1 28.0338428 ## 323 20.4 27.2460664 ## 324 18.5 23.4115884 ## 325 25.0 28.7456889 ## 326 24.6 29.7327822 ## 327 23.0 28.7172151 ## 328 22.2 22.4150038 ## 329 19.3 25.0915453 ## 330 22.6 27.5877525 ## 331 19.8 25.9267782 ## 332 17.1 22.7566900 ## 333 19.4 27.1226797 ## 334 22.2 29.1633053 ## 335 20.7 28.1477381 ## 336 21.1 26.9518366 ## 337 19.5 25.2528971 ## 338 18.5 24.5315597 ## 339 20.6 26.4772725 ## 340 19.0 25.3098448 ## 341 18.7 25.7369525 ## 342 32.7 29.3436397 ## 343 16.5 26.3443946 ## 344 23.9 27.7396130 ## 345 31.2 30.1788725 ## 346 17.5 24.5600336 ## 347 17.2 22.5288992 ## 348 23.1 28.5178981 ## 349 24.5 28.8690756 ## 350 26.6 28.9639884 ## 351 22.9 28.8785669 ## 352 24.1 29.3436397 ## 353 18.6 27.1606448 ## 354 30.1 30.2832766 ## 355 18.2 26.9138715 ## 356 20.6 29.2677094 ## 357 17.8 17.8496972 ## 358 21.7 21.9594223 ## 359 22.7 23.6583618 ## 360 22.6 22.5288992 ## 361 25.0 27.1606448 ## 362 19.9 21.0862243 ## 363 20.8 24.8827371 ## 364 16.8 20.6591167 ## 365 21.9 29.5334653 ## 366 27.5 27.7965607 ## 367 21.9 21.2665587 ## 368 23.1 21.9024746 ## 369 50.0 31.4601956 ## 370 50.0 31.0141053 ## 371 50.0 31.7449340 ## 372 50.0 25.5091618 ## 373 50.0 26.1260951 ## 374 13.8 1.5531660 ## 375 13.8 -1.4840443 ## 376 15.0 21.7980705 ## 377 13.9 12.4966141 ## 378 13.3 14.3948705 ## 379 13.1 12.0695064 ## 380 10.2 13.8823413 ## 381 10.4 18.2198572 ## 382 10.9 14.5467310 ## 383 11.3 12.1549280 ## 384 12.3 11.2437649 ## 385 8.8 5.4825567 ## 386 7.2 5.3117137 ## 387 10.5 7.7130080 ## 388 7.4 4.1917424 ## 389 10.2 5.4920480 ## 390 11.5 14.7650305 ## 391 15.1 18.3147700 ## 392 23.2 16.7487085 ## 393 9.7 10.1807413 ## 394 13.8 20.1560787 ## 395 12.7 19.0361074 ## 396 13.1 18.3052787 ## 397 12.5 16.1697403 ## 398 8.5 15.6477198 ## 399 5.0 5.5205219 ## 400 6.3 6.1089813 ## 401 5.6 9.1461916 ## 402 7.2 15.2680685 ## 403 12.1 15.2775598 ## 404 8.3 15.7900890 ## 405 8.5 8.5672234 ## 406 5.0 12.7433875 ## 407 11.9 12.4017013 ## 408 27.9 23.0414284 ## 409 17.2 9.4973690 ## 410 27.5 15.7805977 ## 411 15.0 24.9586674 ## 412 17.2 14.4138531 ## 413 17.9 1.9328173 ## 414 16.3 15.4958592 ## 415 7.0 -0.5444073 ## 416 7.2 6.9821793 ## 417 7.5 10.0763372 ## 418 10.4 9.2695782 ## 419 8.8 14.9833300 ## 420 8.4 12.9711782 ## 421 16.7 20.2984479 ## 422 14.2 19.6530408 ## 423 20.8 21.1716459 ## 424 13.4 12.4491577 ## 425 11.7 18.2673136 ## 426 8.3 11.4051167 ## 427 10.2 19.6625320 ## 428 10.9 20.7730120 ## 429 11.0 14.1291146 ## 430 9.5 11.6993464 ## 431 14.5 17.8117320 ## 432 14.1 15.8660192 ## 433 16.1 23.1363413 ## 434 14.3 19.1594941 ## 435 11.7 20.1560787 ## 436 13.4 12.4681403 ## 437 9.6 17.4225895 ## 438 8.7 9.4499126 ## 439 8.4 2.2650121 ## 440 12.8 12.8383003 ## 441 10.5 13.5691290 ## 442 17.1 16.0273710 ## 443 18.4 18.8083167 ## 444 15.4 16.6632869 ## 445 10.8 11.9745936 ## 446 11.8 11.7942593 ## 447 14.9 17.6693628 ## 448 12.6 18.9506859 ## 449 14.1 17.3466592 ## 450 13.0 16.2266880 ## 451 13.4 18.0015577 ## 452 15.2 17.7263105 ## 453 16.1 18.1629095 ## 454 17.8 18.6659474 ## 455 14.9 16.7961649 ## 456 14.1 17.3466592 ## 457 12.7 16.5114264 ## 458 13.5 18.4761218 ## 459 14.9 19.1500028 ## 460 20.0 20.6021690 ## 461 16.4 18.9696685 ## 462 17.7 20.6496254 ## 463 19.5 21.2760500 ## 464 20.2 24.7878243 ## 465 21.4 22.0068787 ## 466 19.9 21.1431720 ## 467 19.0 18.2768049 ## 468 19.1 14.3189403 ## 469 19.1 17.3466592 ## 470 20.1 20.5452213 ## 471 19.9 19.0930551 ## 472 19.6 22.3390736 ## 473 23.2 20.9248725 ## 474 29.8 23.4875187 ## 475 13.8 17.3371679 ## 476 13.3 11.6803639 ## 477 16.7 16.8246387 ## 478 12.0 10.9115700 ## 479 14.6 17.4415721 ## 480 21.4 22.1112828 ## 481 23.0 24.3607166 ## 482 23.7 27.2081012 ## 483 25.0 27.9009648 ## 484 21.8 24.6644377 ## 485 20.6 21.8929833 ## 486 21.2 24.5125771 ## 487 19.1 20.3364131 ## 488 20.6 23.6868356 ## 489 15.2 17.4130982 ## 490 7.0 11.8037505 ## 491 8.1 6.3842285 ## 492 13.6 17.4036069 ## 493 20.1 21.8834920 ## 494 21.8 23.1553238 ## 495 24.5 21.6557013 ## 496 23.1 17.8496972 ## 497 19.7 14.4897833 ## 498 18.3 21.1716459 ## 499 21.2 22.2916172 ## 500 17.5 20.2225177 ## 501 16.8 20.9533464 ## 502 22.4 25.3762838 ## 503 20.6 25.9362694 ## 504 23.9 29.2012705 ## 505 22.0 28.4040028 ## 506 11.9 27.0752233 A multiple linear regression model. # specify the model lm_spec_2 &lt;- parsnip::linear_reg() %&gt;% parsnip::set_mode(&quot;regression&quot;) %&gt;% parsnip::set_engine(&quot;lm&quot;) # fit the model lm_fit_2 &lt;- lm_spec_2 %&gt;% parsnip::fit(medv ~ lstat + age, data = Boston) # look at the output summary(lm_fit_2$fit) ## ## Call: ## stats::lm(formula = medv ~ lstat + age, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.981 -3.978 -1.283 1.968 23.158 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 33.22276 0.73085 45.458 &lt; 2e-16 *** ## lstat -1.03207 0.04819 -21.416 &lt; 2e-16 *** ## age 0.03454 0.01223 2.826 0.00491 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.173 on 503 degrees of freedom ## Multiple R-squared: 0.5513, Adjusted R-squared: 0.5495 ## F-statistic: 309 on 2 and 503 DF, p-value: &lt; 2.2e-16 broom::tidy(lm_fit_2) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 33.2 0.731 45.5 2.94e-180 ## 2 lstat -1.03 0.0482 -21.4 8.42e- 73 ## 3 age 0.0345 0.0122 2.83 4.91e- 3 broom::augment(lm_fit_2, new_data = Boston) %&gt;% dplyr::select(medv, .pred) ## medv .pred ## 1 24.0 30.3353500 ## 2 21.6 26.5152022 ## 3 34.7 31.1741833 ## 4 33.4 31.7706097 ## 5 36.2 29.5941382 ## 6 28.7 29.8734360 ## 7 22.9 22.6948012 ## 8 27.1 16.7783585 ## 9 16.5 5.7873823 ## 10 18.9 18.5417468 ## 11 15.0 15.3744895 ## 12 18.9 22.3909364 ## 13 21.7 18.3561926 ## 14 20.4 26.8327143 ## 15 18.2 25.5527337 ## 16 19.9 26.4328949 ## 17 23.1 27.4438985 ## 18 17.5 20.9045872 ## 19 20.2 22.4222018 ## 20 18.2 23.9818587 ## 21 13.6 14.9174789 ## 22 19.6 22.0306073 ## 23 15.2 17.0701529 ## 24 14.5 16.1596713 ## 25 15.6 19.6506652 ## 26 13.9 19.1437584 ## 27 16.6 21.0571789 ## 28 14.8 18.4561530 ## 29 18.4 23.2732685 ## 30 21.0 23.8742999 ## 31 12.7 13.1486332 ## 32 14.5 23.2190203 ## 33 13.2 7.4567764 ## 34 13.1 17.5660145 ## 35 13.5 15.5778323 ## 36 18.9 25.5882607 ## 37 20.0 23.5678806 ## 38 21.0 25.6051093 ## 39 24.7 23.8111450 ## 40 30.8 29.5172909 ## 41 34.9 31.7250653 ## 42 26.6 28.3277273 ## 43 25.3 27.4544348 ## 44 24.7 25.7687086 ## 45 21.2 24.7482793 ## 46 19.3 23.8529391 ## 47 20.0 19.7693168 ## 48 16.6 16.7734125 ## 49 14.4 4.7168035 ## 50 19.4 18.6449988 ## 51 19.7 20.9201146 ## 52 20.5 25.6666473 ## 53 25.0 28.5023241 ## 54 23.4 25.2616714 ## 55 18.9 19.5924563 ## 56 35.4 29.0150318 ## 57 24.7 28.5009578 ## 58 31.6 30.5451354 ## 59 23.3 27.1514649 ## 60 19.6 25.3375812 ## 61 18.7 21.9378941 ## 62 16.0 21.5461317 ## 63 22.2 28.6190453 ## 64 25.0 24.9173335 ## 65 33.0 26.9699967 ## 66 23.5 29.0178896 ## 67 19.4 23.7287074 ## 68 22.0 25.6022540 ## 69 17.4 20.9842147 ## 70 20.9 25.2908410 ## 71 24.2 26.5152524 ## 72 21.7 23.6304490 ## 73 22.8 27.7951879 ## 74 23.4 25.6551385 ## 75 24.1 26.4326017 ## 76 21.4 25.5505628 ## 77 20.0 23.4424530 ## 78 20.8 24.2055471 ## 79 21.2 22.3420654 ## 80 20.3 25.0952594 ## 81 28.0 28.9203532 ## 82 23.9 28.2031469 ## 83 24.8 27.3995875 ## 84 22.9 27.0851462 ## 85 23.9 24.9523892 ## 86 26.6 28.4212902 ## 87 22.5 21.5083085 ## 88 22.2 26.4742203 ## 89 23.6 30.5275598 ## 90 28.7 29.5197175 ## 91 22.6 26.4136173 ## 92 22.0 27.3126249 ## 93 22.9 26.6526576 ## 94 25.0 27.8119461 ## 95 20.6 24.9634318 ## 96 28.4 28.3561673 ## 97 21.4 23.9233890 ## 98 38.7 31.5031216 ## 99 43.8 30.8129619 ## 100 33.2 28.9932773 ## 101 27.5 26.2607673 ## 102 26.5 27.7698060 ## 103 18.6 25.2019582 ## 104 19.3 22.3709342 ## 105 20.1 23.6063456 ## 106 19.5 19.5650288 ## 107 19.5 17.1389858 ## 108 20.4 21.6240921 ## 109 19.8 23.9135345 ## 110 19.4 20.3245380 ## 111 21.7 21.6850812 ## 112 22.8 25.5557619 ## 113 18.8 19.7020982 ## 114 18.7 18.8802387 ## 115 18.5 25.3462773 ## 116 18.3 20.0041706 ## 117 21.2 23.3011196 ## 118 19.2 25.4458167 ## 119 20.4 19.8850578 ## 120 19.3 21.4285982 ## 121 22.0 20.7996757 ## 122 20.3 21.4003210 ## 123 20.5 17.9269402 ## 124 17.3 10.3486992 ## 125 18.8 18.3883428 ## 126 21.4 20.9915446 ## 127 15.7 8.3910102 ## 128 16.2 18.7977584 ## 129 18.0 20.7522060 ## 130 14.3 17.5659719 ## 131 19.2 23.6351317 ## 132 19.6 23.9445818 ## 133 23.0 25.1280488 ## 134 18.4 21.0062999 ## 135 15.6 18.7568166 ## 136 18.1 19.1111317 ## 137 17.4 19.0106975 ## 138 17.1 21.5640431 ## 139 13.3 14.6113128 ## 140 17.8 17.5526656 ## 141 14.0 11.5213341 ## 142 14.4 1.1637151 ## 143 13.4 8.9971155 ## 144 15.6 9.4099429 ## 145 11.8 6.3719086 ## 146 13.8 7.9856883 ## 147 15.6 19.4932528 ## 148 14.6 6.0516690 ## 149 17.8 7.2348378 ## 150 15.4 14.3631476 ## 151 21.5 22.0317579 ## 152 19.6 22.9713239 ## 153 15.3 23.7539913 ## 154 19.4 20.3290153 ## 155 17.0 20.9341403 ## 156 15.6 20.5744531 ## 157 13.1 19.8123417 ## 158 41.3 31.8501844 ## 159 24.3 30.0409935 ## 160 23.3 29.0502077 ## 161 27.0 30.7451892 ## 162 50.0 34.5739079 ## 163 50.0 34.6334429 ## 164 50.0 33.0400063 ## 165 22.7 24.3806527 ## 166 25.0 26.3107914 ## 167 50.0 32.7272722 ## 168 23.8 23.4293598 ## 169 23.8 25.0865104 ## 170 22.3 24.8283654 ## 171 17.4 21.5979056 ## 172 19.1 24.1681398 ## 173 23.1 21.1188473 ## 174 23.6 26.7980396 ## 175 22.6 25.6468156 ## 176 29.4 28.8652527 ## 177 23.2 24.4190401 ## 178 24.6 29.2666037 ## 179 29.9 28.6509449 ## 180 37.2 30.0385243 ## 181 39.8 28.2978656 ## 182 36.2 25.6183705 ## 183 37.9 31.4331781 ## 184 32.5 30.6630499 ## 185 26.4 21.8965236 ## 186 29.6 22.0277094 ## 187 50.0 30.4816320 ## 188 32.0 27.7483148 ## 189 29.8 29.5217681 ## 190 34.9 29.0036857 ## 191 37.0 28.7019141 ## 192 30.5 29.4463246 ## 193 36.4 31.1692399 ## 194 31.1 28.3734446 ## 195 29.1 29.3517338 ## 196 50.0 31.2629357 ## 197 33.3 30.1898827 ## 198 30.3 25.6009730 ## 199 34.6 27.7135148 ## 200 34.9 29.0450563 ## 201 32.9 29.1102217 ## 202 24.1 26.8809937 ## 203 42.3 30.5553734 ## 204 48.5 30.4374513 ## 205 50.0 31.3523675 ## 206 22.6 22.7745140 ## 207 24.4 23.7145462 ## 208 22.5 17.0949757 ## 209 24.4 20.1342058 ## 210 20.0 12.8467312 ## 211 21.7 18.5804700 ## 212 19.3 11.5343848 ## 213 22.4 18.5371869 ## 214 28.1 24.6577395 ## 215 23.7 3.0636690 ## 216 25.0 24.9137512 ## 217 23.3 21.2139972 ## 218 28.7 26.1617394 ## 219 21.5 17.9683508 ## 220 23.0 25.5779375 ## 221 26.7 26.2585487 ## 222 21.7 14.2284673 ## 223 27.5 25.6584148 ## 224 30.1 28.1702220 ## 225 44.8 31.6548184 ## 226 50.0 31.3114632 ## 227 37.6 32.9804712 ## 228 31.6 29.4188971 ## 229 46.7 29.7643055 ## 230 31.5 30.0814316 ## 231 24.3 23.5516312 ## 232 31.7 30.4608602 ## 233 41.7 33.2056512 ## 234 48.3 31.5780111 ## 235 29.0 27.2118071 ## 236 24.0 24.1183314 ## 237 25.1 26.0194683 ## 238 31.5 30.8144509 ## 239 23.7 27.2978747 ## 240 23.3 27.0741863 ## 241 22.0 23.3535779 ## 242 20.1 22.6739468 ## 243 22.2 23.4703468 ## 244 23.7 28.1357705 ## 245 17.6 22.9645454 ## 246 18.5 16.5957874 ## 247 24.3 24.9746099 ## 248 20.5 25.4831762 ## 249 24.5 25.0935948 ## 250 26.2 27.0569167 ## 251 24.4 27.5826324 ## 252 24.8 29.8250790 ## 253 29.6 29.8144600 ## 254 42.8 29.8594103 ## 255 21.9 27.5474889 ## 256 20.9 24.3359232 ## 257 44.0 31.1944437 ## 258 50.0 30.9404725 ## 259 36.0 28.6373803 ## 260 30.1 29.5559213 ## 261 33.8 26.1509499 ## 262 43.1 28.8182066 ## 263 48.8 30.2840423 ## 264 31.0 24.8764292 ## 265 36.5 28.0272666 ## 266 22.8 24.6070285 ## 267 30.7 20.8809175 ## 268 50.0 27.8586411 ## 269 43.5 31.7784561 ## 270 20.7 21.2595015 ## 271 21.1 21.2601859 ## 272 25.2 26.9845014 ## 273 24.4 27.2726232 ## 274 35.2 28.2211461 ## 275 32.4 30.7160672 ## 276 32.0 31.6256939 ## 277 33.2 28.6714183 ## 278 33.1 29.8827790 ## 279 29.1 26.9110608 ## 280 35.1 29.3295557 ## 281 45.4 31.5702926 ## 282 35.4 29.7706152 ## 283 46.0 31.8330878 ## 284 50.0 30.8181235 ## 285 32.2 25.8395445 ## 286 22.0 25.8308006 ## 287 20.1 20.9662607 ## 288 23.2 26.9350288 ## 289 22.3 26.9542613 ## 290 24.8 24.1988538 ## 291 28.5 30.7497593 ## 292 37.3 30.5054746 ## 293 27.9 29.1803758 ## 294 23.9 25.0032281 ## 295 21.7 23.9504730 ## 296 28.6 27.8260196 ## 297 27.1 27.3575351 ## 298 20.3 18.8783661 ## 299 22.5 28.7877210 ## 300 29.0 28.6761989 ## 301 24.8 28.5955060 ## 302 22.0 24.8137005 ## 303 26.4 24.9103419 ## 304 33.1 28.8183421 ## 305 36.1 27.4902977 ## 306 28.4 26.0134143 ## 307 33.4 29.0290149 ## 308 28.2 27.8797512 ## 309 22.8 31.3870772 ## 310 20.3 25.5825877 ## 311 16.1 21.4831899 ## 312 22.1 28.8749316 ## 313 19.4 24.2497252 ## 314 21.6 27.9296901 ## 315 23.8 26.6608850 ## 316 16.2 24.0380672 ## 317 17.8 17.1790327 ## 318 19.8 19.2484167 ## 319 23.1 24.8519098 ## 320 21.0 22.1157348 ## 321 23.8 27.5985358 ## 322 23.1 28.0082071 ## 323 20.4 26.9995951 ## 324 18.5 23.6729199 ## 325 25.0 28.2917289 ## 326 24.6 28.4876540 ## 327 23.0 27.8738702 ## 328 22.2 21.5321912 ## 329 19.3 23.8242809 ## 330 22.6 26.2415399 ## 331 19.8 24.9535850 ## 332 17.1 21.3752075 ## 333 19.4 25.9465468 ## 334 22.2 28.6767504 ## 335 20.7 27.5862548 ## 336 21.1 26.1476710 ## 337 19.5 24.7078915 ## 338 18.5 24.3829591 ## 339 20.6 25.7283609 ## 340 19.0 24.7387257 ## 341 18.7 25.6556874 ## 342 32.7 29.2597400 ## 343 16.5 26.3576645 ## 344 23.9 27.7608089 ## 345 31.2 29.4356204 ## 346 17.5 24.0304790 ## 347 17.2 21.9531207 ## 348 23.1 27.6156826 ## 349 24.5 28.0666367 ## 350 26.6 28.3356564 ## 351 22.9 28.5847592 ## 352 24.1 28.7968459 ## 353 18.6 25.8220167 ## 354 30.1 29.8255026 ## 355 18.2 25.6711296 ## 356 20.6 28.1477532 ## 357 17.8 18.4229724 ## 358 21.7 22.6707455 ## 359 22.7 24.2556113 ## 360 22.6 22.9549065 ## 361 25.0 28.2228482 ## 362 19.9 21.7246968 ## 363 20.8 26.0291472 ## 364 16.8 21.1877229 ## 365 21.9 30.6268435 ## 366 27.5 28.9108797 ## 367 21.9 21.9311532 ## 368 23.1 22.9197204 ## 369 50.0 33.3126509 ## 370 50.0 32.7170368 ## 371 50.0 33.5359106 ## 372 50.0 26.8415810 ## 373 50.0 27.1531644 ## 374 13.8 0.7921704 ## 375 13.8 -2.5104490 ## 376 15.0 22.7336498 ## 377 13.9 12.4604739 ## 378 13.3 14.7146049 ## 379 13.1 12.0962216 ## 380 10.2 14.1987411 ## 381 10.4 18.6354853 ## 382 10.9 14.8900992 ## 383 11.3 12.3203763 ## 384 12.3 11.3295905 ## 385 8.8 4.7609441 ## 386 7.2 4.8135277 ## 387 10.5 7.4902954 ## 388 7.4 3.2986055 ## 389 10.2 5.0752550 ## 390 11.5 15.1205661 ## 391 15.1 18.9148682 ## 392 23.2 16.7110622 ## 393 9.7 10.0700406 ## 394 13.8 20.7650862 ## 395 12.7 19.6197884 ## 396 13.1 18.9667274 ## 397 12.5 16.5478489 ## 398 8.5 16.0803898 ## 399 5.0 5.1062170 ## 400 6.3 4.9792152 ## 401 5.6 9.0487189 ## 402 7.2 15.7055612 ## 403 12.1 15.7158819 ## 404 8.3 16.1350215 ## 405 8.5 7.9148098 ## 406 5.0 12.9602588 ## 407 11.9 12.5887141 ## 408 27.9 24.1582027 ## 409 17.2 9.3580412 ## 410 27.5 16.2628782 ## 411 15.0 26.2429812 ## 412 17.2 14.7766995 ## 413 17.9 1.2049978 ## 414 16.3 15.9532576 ## 415 7.0 -1.4887011 ## 416 7.2 6.6956026 ## 417 7.5 9.7423382 ## 418 10.4 8.8063545 ## 419 8.8 15.3959406 ## 420 8.4 12.3961633 ## 421 16.7 21.1755246 ## 422 14.2 20.3113595 ## 423 20.8 21.6966778 ## 424 13.4 12.1256069 ## 425 11.7 17.9512943 ## 426 8.3 11.3461382 ## 427 10.2 19.0919018 ## 428 10.9 20.9557644 ## 429 11.0 13.7105579 ## 430 9.5 11.6729883 ## 431 14.5 17.9913386 ## 432 14.1 16.1588616 ## 433 16.1 23.3908922 ## 434 14.3 19.5190558 ## 435 11.7 20.8479926 ## 436 13.4 12.4744195 ## 437 9.6 17.8169097 ## 438 8.7 9.3789809 ## 439 8.4 1.1482353 ## 440 12.8 12.8527452 ## 441 10.5 13.5956215 ## 442 17.1 16.4344919 ## 443 18.4 19.5551769 ## 444 15.4 17.2227020 ## 445 10.8 12.0068325 ## 446 11.8 11.7485597 ## 447 14.9 18.1923350 ## 448 12.6 19.5925364 ## 449 14.1 17.9208837 ## 450 13.0 16.6892250 ## 451 13.4 18.4222905 ## 452 15.2 18.3164389 ## 453 16.1 18.5701067 ## 454 17.8 19.3761856 ## 455 14.9 17.1633800 ## 456 14.1 17.4994427 ## 457 12.7 16.6395845 ## 458 13.5 18.5134294 ## 459 14.9 19.3636489 ## 460 20.0 20.9668948 ## 461 16.4 19.3851852 ## 462 17.7 21.1566756 ## 463 19.5 21.6513014 ## 464 20.2 25.7083110 ## 465 21.4 21.8380139 ## 466 19.9 20.3046688 ## 467 19.0 18.4486901 ## 468 19.1 14.4834987 ## 469 19.1 16.9640055 ## 470 20.1 19.9480925 ## 471 19.9 19.3120881 ## 472 19.6 23.0732096 ## 473 23.2 20.9930813 ## 474 29.8 23.5240384 ## 475 13.8 17.7965667 ## 476 13.3 11.7145267 ## 477 16.7 17.1770698 ## 478 12.0 10.8750967 ## 479 14.6 17.9550019 ## 480 21.4 22.7322434 ## 481 23.0 24.3733629 ## 482 23.7 27.8219208 ## 483 25.0 28.6478740 ## 484 21.8 23.8607429 ## 485 20.6 20.9023737 ## 486 21.2 24.0963263 ## 487 19.1 20.5190117 ## 488 20.6 23.2433343 ## 489 15.2 17.7858624 ## 490 7.0 11.8797855 ## 491 8.1 5.9763107 ## 492 13.6 17.9862622 ## 493 20.1 22.3290975 ## 494 21.8 22.6930114 ## 495 24.5 20.6685376 ## 496 23.1 16.0532308 ## 497 19.7 13.9231134 ## 498 18.3 21.1094241 ## 499 21.2 22.1441800 ## 500 17.5 20.1775341 ## 501 16.8 21.1864018 ## 502 22.4 25.6296713 ## 503 20.6 26.5011287 ## 504 23.9 30.5454286 ## 505 22.0 29.6197657 ## 506 11.9 27.8812428 A more complicated linear regression model. # fit the model using all available varaibles as predictors lm_fit_3 &lt;- lm_spec_2 %&gt;% fit(medv ~ ., Boston) # look at modle fit output tidy(lm_fit_3) %&gt;% filter(p.value &lt; 0.05) ## # A tibble: 12 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 36.5 5.10 7.14 3.28e-12 ## 2 crim -0.108 0.0329 -3.29 1.09e- 3 ## 3 zn 0.0464 0.0137 3.38 7.78e- 4 ## 4 chas 2.69 0.862 3.12 1.93e- 3 ## 5 nox -17.8 3.82 -4.65 4.25e- 6 ## 6 rm 3.81 0.418 9.12 1.98e-18 ## 7 dis -1.48 0.199 -7.40 6.01e-13 ## 8 rad 0.306 0.0663 4.61 5.07e- 6 ## 9 tax -0.0123 0.00376 -3.28 1.11e- 3 ## 10 ptratio -0.953 0.131 -7.28 1.31e-12 ## 11 black 0.00931 0.00269 3.47 5.73e- 4 ## 12 lstat -0.525 0.0507 -10.3 7.78e-23 Interaction terms in formula # fit a model with an interaction specfied in the formula lm_fit_4 &lt;- lm_spec_2 %&gt;% parsnip::fit(medv ~ lstat * age, data = Boston) # look at the model fit output in a tidy form broom::tidy(lm_fit_4) ## # A tibble: 4 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 36.1 1.47 24.6 4.91e-88 ## 2 lstat -1.39 0.167 -8.31 8.78e-16 ## 3 age -0.000721 0.0199 -0.0363 9.71e- 1 ## 4 lstat:age 0.00416 0.00185 2.24 2.52e- 2 Interactions terms using recipes. # specify the interaction using recipes rec_spec &lt;- recipes::recipe(medv ~ lstat + age, data = Boston) %&gt;% recipes::step_interact(terms = ~ lstat:age) # create a modelling workflow lm_wf &lt;- workflows::workflow() %&gt;% add_model(lm_spec) %&gt;% add_recipe(rec_spec) # run the workflow lm_wf_out &lt;- lm_wf %&gt;% parsnip::fit(Boston) tidy(lm_wf_out) ## # A tibble: 4 x 3 ## term estimate std.error ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 36.0 1.49 ## 2 lstat -1.39 0.168 ## 3 age -0.0000935 0.0205 ## 4 lstat_x_age 0.00409 0.00183 glance(lm_wf_out) ## # A tibble: 1 x 4 ## algorithm pss nobs sigma ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 sampling 4000 506 6.15 # get the actual model fit lm_wf_out$fit$fit$fit ## stan_glm ## family: gaussian [identity] ## formula: ..y ~ . ## observations: 506 ## predictors: 4 ## ------ ## Median MAD_SD ## (Intercept) 36.0 1.5 ## lstat -1.4 0.2 ## age 0.0 0.0 ## lstat_x_age 0.0 0.0 ## ## Auxiliary parameter(s): ## Median MAD_SD ## sigma 6.2 0.2 ## ## ------ ## * For help interpreting the printed output see ?print.stanreg ## * For info on the priors used see ?prior_summary.stanreg Applying non-linear transformations to predictors # create a new recipe rec_spec_pow2 &lt;- recipes::recipe(medv ~ lstat, data = Boston) %&gt;% step_mutate(lstat = lstat ^ 2) # create a new workflow including the new recipe lm_wf_pow2 &lt;- workflows::workflow() %&gt;% add_model(lm_spec) %&gt;% add_recipe(rec_spec_pow2) # fit the workflow lm_wf_pow2 %&gt;% fit(data = Boston) ## == Workflow [trained] ========================================================== ## Preprocessor: Recipe ## Model: linear_reg() ## ## -- Preprocessor ---------------------------------------------------------------- ## 1 Recipe Step ## ## * step_mutate() ## ## -- Model ----------------------------------------------------------------------- ## stan_glm ## family: gaussian [identity] ## formula: ..y ~ . ## observations: 506 ## predictors: 2 ## ------ ## Median MAD_SD ## (Intercept) 27.6 0.4 ## lstat 0.0 0.0 ## ## Auxiliary parameter(s): ## Median MAD_SD ## sigma 7.2 0.2 ## ## ------ ## * For help interpreting the printed output see ?print.stanreg ## * For info on the priors used see ?prior_summary.stanreg Using predefined steps from recipes. # create different recipe rec_spec_log &lt;- recipes::recipe(medv ~ lstat, data = Boston) %&gt;% step_log(lstat) # create a workflow incorporating this recipe lm_wf_log &lt;- workflows::workflow() %&gt;% add_model(lm_spec) %&gt;% add_recipe(rec_spec_log) # run the workflow lm_wf_log %&gt;% fit(data = Boston) ## == Workflow [trained] ========================================================== ## Preprocessor: Recipe ## Model: linear_reg() ## ## -- Preprocessor ---------------------------------------------------------------- ## 1 Recipe Step ## ## * step_log() ## ## -- Model ----------------------------------------------------------------------- ## stan_glm ## family: gaussian [identity] ## formula: ..y ~ . ## observations: 506 ## predictors: 2 ## ------ ## Median MAD_SD ## (Intercept) 52.1 0.9 ## lstat -12.5 0.4 ## ## Auxiliary parameter(s): ## Median MAD_SD ## sigma 5.3 0.2 ## ## ------ ## * For help interpreting the printed output see ?print.stanreg ## * For info on the priors used see ?prior_summary.stanreg Dealing with qualitative predictors. Many models convert qualitative predictors to dummy variables automatically, so qualitative predictors can just be added the formula. But if not, dummy variables can be created in the recipe. # look at the dataset used in the lab Carseats ## Sales CompPrice Income Advertising Population Price ShelveLoc Age Education ## 1 9.50 138 73 11 276 120 Bad 42 17 ## 2 11.22 111 48 16 260 83 Good 65 10 ## 3 10.06 113 35 10 269 80 Medium 59 12 ## 4 7.40 117 100 4 466 97 Medium 55 14 ## 5 4.15 141 64 3 340 128 Bad 38 13 ## 6 10.81 124 113 13 501 72 Bad 78 16 ## 7 6.63 115 105 0 45 108 Medium 71 15 ## 8 11.85 136 81 15 425 120 Good 67 10 ## 9 6.54 132 110 0 108 124 Medium 76 10 ## 10 4.69 132 113 0 131 124 Medium 76 17 ## 11 9.01 121 78 9 150 100 Bad 26 10 ## 12 11.96 117 94 4 503 94 Good 50 13 ## 13 3.98 122 35 2 393 136 Medium 62 18 ## 14 10.96 115 28 11 29 86 Good 53 18 ## 15 11.17 107 117 11 148 118 Good 52 18 ## 16 8.71 149 95 5 400 144 Medium 76 18 ## 17 7.58 118 32 0 284 110 Good 63 13 ## 18 12.29 147 74 13 251 131 Good 52 10 ## 19 13.91 110 110 0 408 68 Good 46 17 ## 20 8.73 129 76 16 58 121 Medium 69 12 ## 21 6.41 125 90 2 367 131 Medium 35 18 ## 22 12.13 134 29 12 239 109 Good 62 18 ## 23 5.08 128 46 6 497 138 Medium 42 13 ## 24 5.87 121 31 0 292 109 Medium 79 10 ## 25 10.14 145 119 16 294 113 Bad 42 12 ## 26 14.90 139 32 0 176 82 Good 54 11 ## 27 8.33 107 115 11 496 131 Good 50 11 ## 28 5.27 98 118 0 19 107 Medium 64 17 ## 29 2.99 103 74 0 359 97 Bad 55 11 ## 30 7.81 104 99 15 226 102 Bad 58 17 ## 31 13.55 125 94 0 447 89 Good 30 12 ## 32 8.25 136 58 16 241 131 Medium 44 18 ## 33 6.20 107 32 12 236 137 Good 64 10 ## 34 8.77 114 38 13 317 128 Good 50 16 ## 35 2.67 115 54 0 406 128 Medium 42 17 ## 36 11.07 131 84 11 29 96 Medium 44 17 ## 37 8.89 122 76 0 270 100 Good 60 18 ## 38 4.95 121 41 5 412 110 Medium 54 10 ## 39 6.59 109 73 0 454 102 Medium 65 15 ## 40 3.24 130 60 0 144 138 Bad 38 10 ## 41 2.07 119 98 0 18 126 Bad 73 17 ## 42 7.96 157 53 0 403 124 Bad 58 16 ## 43 10.43 77 69 0 25 24 Medium 50 18 ## 44 4.12 123 42 11 16 134 Medium 59 13 ## 45 4.16 85 79 6 325 95 Medium 69 13 ## 46 4.56 141 63 0 168 135 Bad 44 12 ## 47 12.44 127 90 14 16 70 Medium 48 15 ## 48 4.38 126 98 0 173 108 Bad 55 16 ## 49 3.91 116 52 0 349 98 Bad 69 18 ## 50 10.61 157 93 0 51 149 Good 32 17 ## 51 1.42 99 32 18 341 108 Bad 80 16 ## 52 4.42 121 90 0 150 108 Bad 75 16 ## 53 7.91 153 40 3 112 129 Bad 39 18 ## 54 6.92 109 64 13 39 119 Medium 61 17 ## 55 4.90 134 103 13 25 144 Medium 76 17 ## 56 6.85 143 81 5 60 154 Medium 61 18 ## 57 11.91 133 82 0 54 84 Medium 50 17 ## 58 0.91 93 91 0 22 117 Bad 75 11 ## 59 5.42 103 93 15 188 103 Bad 74 16 ## 60 5.21 118 71 4 148 114 Medium 80 13 ## 61 8.32 122 102 19 469 123 Bad 29 13 ## 62 7.32 105 32 0 358 107 Medium 26 13 ## 63 1.82 139 45 0 146 133 Bad 77 17 ## 64 8.47 119 88 10 170 101 Medium 61 13 ## 65 7.80 100 67 12 184 104 Medium 32 16 ## 66 4.90 122 26 0 197 128 Medium 55 13 ## 67 8.85 127 92 0 508 91 Medium 56 18 ## 68 9.01 126 61 14 152 115 Medium 47 16 ## 69 13.39 149 69 20 366 134 Good 60 13 ## 70 7.99 127 59 0 339 99 Medium 65 12 ## 71 9.46 89 81 15 237 99 Good 74 12 ## 72 6.50 148 51 16 148 150 Medium 58 17 ## 73 5.52 115 45 0 432 116 Medium 25 15 ## 74 12.61 118 90 10 54 104 Good 31 11 ## 75 6.20 150 68 5 125 136 Medium 64 13 ## 76 8.55 88 111 23 480 92 Bad 36 16 ## 77 10.64 102 87 10 346 70 Medium 64 15 ## 78 7.70 118 71 12 44 89 Medium 67 18 ## 79 4.43 134 48 1 139 145 Medium 65 12 ## 80 9.14 134 67 0 286 90 Bad 41 13 ## 81 8.01 113 100 16 353 79 Bad 68 11 ## 82 7.52 116 72 0 237 128 Good 70 13 ## 83 11.62 151 83 4 325 139 Good 28 17 ## 84 4.42 109 36 7 468 94 Bad 56 11 ## 85 2.23 111 25 0 52 121 Bad 43 18 ## 86 8.47 125 103 0 304 112 Medium 49 13 ## 87 8.70 150 84 9 432 134 Medium 64 15 ## 88 11.70 131 67 7 272 126 Good 54 16 ## 89 6.56 117 42 7 144 111 Medium 62 10 ## 90 7.95 128 66 3 493 119 Medium 45 16 ## 91 5.33 115 22 0 491 103 Medium 64 11 ## 92 4.81 97 46 11 267 107 Medium 80 15 ## 93 4.53 114 113 0 97 125 Medium 29 12 ## 94 8.86 145 30 0 67 104 Medium 55 17 ## 95 8.39 115 97 5 134 84 Bad 55 11 ## 96 5.58 134 25 10 237 148 Medium 59 13 ## 97 9.48 147 42 10 407 132 Good 73 16 ## 98 7.45 161 82 5 287 129 Bad 33 16 ## 99 12.49 122 77 24 382 127 Good 36 16 ## 100 4.88 121 47 3 220 107 Bad 56 16 ## 101 4.11 113 69 11 94 106 Medium 76 12 ## 102 6.20 128 93 0 89 118 Medium 34 18 ## 103 5.30 113 22 0 57 97 Medium 65 16 ## 104 5.07 123 91 0 334 96 Bad 78 17 ## 105 4.62 121 96 0 472 138 Medium 51 12 ## 106 5.55 104 100 8 398 97 Medium 61 11 ## 107 0.16 102 33 0 217 139 Medium 70 18 ## 108 8.55 134 107 0 104 108 Medium 60 12 ## 109 3.47 107 79 2 488 103 Bad 65 16 ## 110 8.98 115 65 0 217 90 Medium 60 17 ## 111 9.00 128 62 7 125 116 Medium 43 14 ## 112 6.62 132 118 12 272 151 Medium 43 14 ## 113 6.67 116 99 5 298 125 Good 62 12 ## 114 6.01 131 29 11 335 127 Bad 33 12 ## 115 9.31 122 87 9 17 106 Medium 65 13 ## 116 8.54 139 35 0 95 129 Medium 42 13 ## 117 5.08 135 75 0 202 128 Medium 80 10 ## 118 8.80 145 53 0 507 119 Medium 41 12 ## 119 7.57 112 88 2 243 99 Medium 62 11 ## 120 7.37 130 94 8 137 128 Medium 64 12 ## 121 6.87 128 105 11 249 131 Medium 63 13 ## 122 11.67 125 89 10 380 87 Bad 28 10 ## 123 6.88 119 100 5 45 108 Medium 75 10 ## 124 8.19 127 103 0 125 155 Good 29 15 ## 125 8.87 131 113 0 181 120 Good 63 14 ## 126 9.34 89 78 0 181 49 Medium 43 15 ## 127 11.27 153 68 2 60 133 Good 59 16 ## 128 6.52 125 48 3 192 116 Medium 51 14 ## 129 4.96 133 100 3 350 126 Bad 55 13 ## 130 4.47 143 120 7 279 147 Bad 40 10 ## 131 8.41 94 84 13 497 77 Medium 51 12 ## 132 6.50 108 69 3 208 94 Medium 77 16 ## 133 9.54 125 87 9 232 136 Good 72 10 ## 134 7.62 132 98 2 265 97 Bad 62 12 ## 135 3.67 132 31 0 327 131 Medium 76 16 ## 136 6.44 96 94 14 384 120 Medium 36 18 ## 137 5.17 131 75 0 10 120 Bad 31 18 ## 138 6.52 128 42 0 436 118 Medium 80 11 ## 139 10.27 125 103 12 371 109 Medium 44 10 ## 140 12.30 146 62 10 310 94 Medium 30 13 ## 141 6.03 133 60 10 277 129 Medium 45 18 ## 142 6.53 140 42 0 331 131 Bad 28 15 ## 143 7.44 124 84 0 300 104 Medium 77 15 ## 144 0.53 122 88 7 36 159 Bad 28 17 ## 145 9.09 132 68 0 264 123 Good 34 11 ## 146 8.77 144 63 11 27 117 Medium 47 17 ## 147 3.90 114 83 0 412 131 Bad 39 14 ## 148 10.51 140 54 9 402 119 Good 41 16 ## 149 7.56 110 119 0 384 97 Medium 72 14 ## 150 11.48 121 120 13 140 87 Medium 56 11 ## 151 10.49 122 84 8 176 114 Good 57 10 ## 152 10.77 111 58 17 407 103 Good 75 17 ## 153 7.64 128 78 0 341 128 Good 45 13 ## 154 5.93 150 36 7 488 150 Medium 25 17 ## 155 6.89 129 69 10 289 110 Medium 50 16 ## 156 7.71 98 72 0 59 69 Medium 65 16 ## 157 7.49 146 34 0 220 157 Good 51 16 ## 158 10.21 121 58 8 249 90 Medium 48 13 ## 159 12.53 142 90 1 189 112 Good 39 10 ## 160 9.32 119 60 0 372 70 Bad 30 18 ## 161 4.67 111 28 0 486 111 Medium 29 12 ## 162 2.93 143 21 5 81 160 Medium 67 12 ## 163 3.63 122 74 0 424 149 Medium 51 13 ## 164 5.68 130 64 0 40 106 Bad 39 17 ## 165 8.22 148 64 0 58 141 Medium 27 13 ## 166 0.37 147 58 7 100 191 Bad 27 15 ## 167 6.71 119 67 17 151 137 Medium 55 11 ## 168 6.71 106 73 0 216 93 Medium 60 13 ## 169 7.30 129 89 0 425 117 Medium 45 10 ## 170 11.48 104 41 15 492 77 Good 73 18 ## 171 8.01 128 39 12 356 118 Medium 71 10 ## 172 12.49 93 106 12 416 55 Medium 75 15 ## 173 9.03 104 102 13 123 110 Good 35 16 ## 174 6.38 135 91 5 207 128 Medium 66 18 ## 175 0.00 139 24 0 358 185 Medium 79 15 ## 176 7.54 115 89 0 38 122 Medium 25 12 ## 177 5.61 138 107 9 480 154 Medium 47 11 ## 178 10.48 138 72 0 148 94 Medium 27 17 ## 179 10.66 104 71 14 89 81 Medium 25 14 ## 180 7.78 144 25 3 70 116 Medium 77 18 ## 181 4.94 137 112 15 434 149 Bad 66 13 ## 182 7.43 121 83 0 79 91 Medium 68 11 ## 183 4.74 137 60 4 230 140 Bad 25 13 ## 184 5.32 118 74 6 426 102 Medium 80 18 ## 185 9.95 132 33 7 35 97 Medium 60 11 ## 186 10.07 130 100 11 449 107 Medium 64 10 ## 187 8.68 120 51 0 93 86 Medium 46 17 ## 188 6.03 117 32 0 142 96 Bad 62 17 ## 189 8.07 116 37 0 426 90 Medium 76 15 ## 190 12.11 118 117 18 509 104 Medium 26 15 ## 191 8.79 130 37 13 297 101 Medium 37 13 ## 192 6.67 156 42 13 170 173 Good 74 14 ## 193 7.56 108 26 0 408 93 Medium 56 14 ## 194 13.28 139 70 7 71 96 Good 61 10 ## 195 7.23 112 98 18 481 128 Medium 45 11 ## 196 4.19 117 93 4 420 112 Bad 66 11 ## 197 4.10 130 28 6 410 133 Bad 72 16 ## 198 2.52 124 61 0 333 138 Medium 76 16 ## 199 3.62 112 80 5 500 128 Medium 69 10 ## 200 6.42 122 88 5 335 126 Medium 64 14 ## 201 5.56 144 92 0 349 146 Medium 62 12 ## 202 5.94 138 83 0 139 134 Medium 54 18 ## 203 4.10 121 78 4 413 130 Bad 46 10 ## 204 2.05 131 82 0 132 157 Bad 25 14 ## 205 8.74 155 80 0 237 124 Medium 37 14 ## 206 5.68 113 22 1 317 132 Medium 28 12 ## 207 4.97 162 67 0 27 160 Medium 77 17 ## 208 8.19 111 105 0 466 97 Bad 61 10 ## 209 7.78 86 54 0 497 64 Bad 33 12 ## 210 3.02 98 21 11 326 90 Bad 76 11 ## 211 4.36 125 41 2 357 123 Bad 47 14 ## 212 9.39 117 118 14 445 120 Medium 32 15 ## 213 12.04 145 69 19 501 105 Medium 45 11 ## 214 8.23 149 84 5 220 139 Medium 33 10 ## 215 4.83 115 115 3 48 107 Medium 73 18 ## 216 2.34 116 83 15 170 144 Bad 71 11 ## 217 5.73 141 33 0 243 144 Medium 34 17 ## 218 4.34 106 44 0 481 111 Medium 70 14 ## 219 9.70 138 61 12 156 120 Medium 25 14 ## 220 10.62 116 79 19 359 116 Good 58 17 ## 221 10.59 131 120 15 262 124 Medium 30 10 ## 222 6.43 124 44 0 125 107 Medium 80 11 ## 223 7.49 136 119 6 178 145 Medium 35 13 ## 224 3.45 110 45 9 276 125 Medium 62 14 ## 225 4.10 134 82 0 464 141 Medium 48 13 ## 226 6.68 107 25 0 412 82 Bad 36 14 ## 227 7.80 119 33 0 245 122 Good 56 14 ## 228 8.69 113 64 10 68 101 Medium 57 16 ## 229 5.40 149 73 13 381 163 Bad 26 11 ## 230 11.19 98 104 0 404 72 Medium 27 18 ## 231 5.16 115 60 0 119 114 Bad 38 14 ## 232 8.09 132 69 0 123 122 Medium 27 11 ## 233 13.14 137 80 10 24 105 Good 61 15 ## 234 8.65 123 76 18 218 120 Medium 29 14 ## 235 9.43 115 62 11 289 129 Good 56 16 ## 236 5.53 126 32 8 95 132 Medium 50 17 ## 237 9.32 141 34 16 361 108 Medium 69 10 ## 238 9.62 151 28 8 499 135 Medium 48 10 ## 239 7.36 121 24 0 200 133 Good 73 13 ## 240 3.89 123 105 0 149 118 Bad 62 16 ## 241 10.31 159 80 0 362 121 Medium 26 18 ## 242 12.01 136 63 0 160 94 Medium 38 12 ## 243 4.68 124 46 0 199 135 Medium 52 14 ## 244 7.82 124 25 13 87 110 Medium 57 10 ## 245 8.78 130 30 0 391 100 Medium 26 18 ## 246 10.00 114 43 0 199 88 Good 57 10 ## 247 6.90 120 56 20 266 90 Bad 78 18 ## 248 5.04 123 114 0 298 151 Bad 34 16 ## 249 5.36 111 52 0 12 101 Medium 61 11 ## 250 5.05 125 67 0 86 117 Bad 65 11 ## 251 9.16 137 105 10 435 156 Good 72 14 ## 252 3.72 139 111 5 310 132 Bad 62 13 ## 253 8.31 133 97 0 70 117 Medium 32 16 ## 254 5.64 124 24 5 288 122 Medium 57 12 ## 255 9.58 108 104 23 353 129 Good 37 17 ## 256 7.71 123 81 8 198 81 Bad 80 15 ## 257 4.20 147 40 0 277 144 Medium 73 10 ## 258 8.67 125 62 14 477 112 Medium 80 13 ## 259 3.47 108 38 0 251 81 Bad 72 14 ## 260 5.12 123 36 10 467 100 Bad 74 11 ## 261 7.67 129 117 8 400 101 Bad 36 10 ## 262 5.71 121 42 4 188 118 Medium 54 15 ## 263 6.37 120 77 15 86 132 Medium 48 18 ## 264 7.77 116 26 6 434 115 Medium 25 17 ## 265 6.95 128 29 5 324 159 Good 31 15 ## 266 5.31 130 35 10 402 129 Bad 39 17 ## 267 9.10 128 93 12 343 112 Good 73 17 ## 268 5.83 134 82 7 473 112 Bad 51 12 ## 269 6.53 123 57 0 66 105 Medium 39 11 ## 270 5.01 159 69 0 438 166 Medium 46 17 ## 271 11.99 119 26 0 284 89 Good 26 10 ## 272 4.55 111 56 0 504 110 Medium 62 16 ## 273 12.98 113 33 0 14 63 Good 38 12 ## 274 10.04 116 106 8 244 86 Medium 58 12 ## 275 7.22 135 93 2 67 119 Medium 34 11 ## 276 6.67 107 119 11 210 132 Medium 53 11 ## 277 6.93 135 69 14 296 130 Medium 73 15 ## 278 7.80 136 48 12 326 125 Medium 36 16 ## 279 7.22 114 113 2 129 151 Good 40 15 ## 280 3.42 141 57 13 376 158 Medium 64 18 ## 281 2.86 121 86 10 496 145 Bad 51 10 ## 282 11.19 122 69 7 303 105 Good 45 16 ## 283 7.74 150 96 0 80 154 Good 61 11 ## 284 5.36 135 110 0 112 117 Medium 80 16 ## 285 6.97 106 46 11 414 96 Bad 79 17 ## 286 7.60 146 26 11 261 131 Medium 39 10 ## 287 7.53 117 118 11 429 113 Medium 67 18 ## 288 6.88 95 44 4 208 72 Bad 44 17 ## 289 6.98 116 40 0 74 97 Medium 76 15 ## 290 8.75 143 77 25 448 156 Medium 43 17 ## 291 9.49 107 111 14 400 103 Medium 41 11 ## 292 6.64 118 70 0 106 89 Bad 39 17 ## 293 11.82 113 66 16 322 74 Good 76 15 ## 294 11.28 123 84 0 74 89 Good 59 10 ## 295 12.66 148 76 3 126 99 Good 60 11 ## 296 4.21 118 35 14 502 137 Medium 79 10 ## 297 8.21 127 44 13 160 123 Good 63 18 ## 298 3.07 118 83 13 276 104 Bad 75 10 ## 299 10.98 148 63 0 312 130 Good 63 15 ## 300 9.40 135 40 17 497 96 Medium 54 17 ## 301 8.57 116 78 1 158 99 Medium 45 11 ## 302 7.41 99 93 0 198 87 Medium 57 16 ## 303 5.28 108 77 13 388 110 Bad 74 14 ## 304 10.01 133 52 16 290 99 Medium 43 11 ## 305 11.93 123 98 12 408 134 Good 29 10 ## 306 8.03 115 29 26 394 132 Medium 33 13 ## 307 4.78 131 32 1 85 133 Medium 48 12 ## 308 5.90 138 92 0 13 120 Bad 61 12 ## 309 9.24 126 80 19 436 126 Medium 52 10 ## 310 11.18 131 111 13 33 80 Bad 68 18 ## 311 9.53 175 65 29 419 166 Medium 53 12 ## 312 6.15 146 68 12 328 132 Bad 51 14 ## 313 6.80 137 117 5 337 135 Bad 38 10 ## 314 9.33 103 81 3 491 54 Medium 66 13 ## 315 7.72 133 33 10 333 129 Good 71 14 ## 316 6.39 131 21 8 220 171 Good 29 14 ## 317 15.63 122 36 5 369 72 Good 35 10 ## 318 6.41 142 30 0 472 136 Good 80 15 ## 319 10.08 116 72 10 456 130 Good 41 14 ## 320 6.97 127 45 19 459 129 Medium 57 11 ## 321 5.86 136 70 12 171 152 Medium 44 18 ## 322 7.52 123 39 5 499 98 Medium 34 15 ## 323 9.16 140 50 10 300 139 Good 60 15 ## 324 10.36 107 105 18 428 103 Medium 34 12 ## 325 2.66 136 65 4 133 150 Bad 53 13 ## 326 11.70 144 69 11 131 104 Medium 47 11 ## 327 4.69 133 30 0 152 122 Medium 53 17 ## 328 6.23 112 38 17 316 104 Medium 80 16 ## 329 3.15 117 66 1 65 111 Bad 55 11 ## 330 11.27 100 54 9 433 89 Good 45 12 ## 331 4.99 122 59 0 501 112 Bad 32 14 ## 332 10.10 135 63 15 213 134 Medium 32 10 ## 333 5.74 106 33 20 354 104 Medium 61 12 ## 334 5.87 136 60 7 303 147 Medium 41 10 ## 335 7.63 93 117 9 489 83 Bad 42 13 ## 336 6.18 120 70 15 464 110 Medium 72 15 ## 337 5.17 138 35 6 60 143 Bad 28 18 ## 338 8.61 130 38 0 283 102 Medium 80 15 ## 339 5.97 112 24 0 164 101 Medium 45 11 ## 340 11.54 134 44 4 219 126 Good 44 15 ## 341 7.50 140 29 0 105 91 Bad 43 16 ## 342 7.38 98 120 0 268 93 Medium 72 10 ## 343 7.81 137 102 13 422 118 Medium 71 10 ## 344 5.99 117 42 10 371 121 Bad 26 14 ## 345 8.43 138 80 0 108 126 Good 70 13 ## 346 4.81 121 68 0 279 149 Good 79 12 ## 347 8.97 132 107 0 144 125 Medium 33 13 ## 348 6.88 96 39 0 161 112 Good 27 14 ## 349 12.57 132 102 20 459 107 Good 49 11 ## 350 9.32 134 27 18 467 96 Medium 49 14 ## 351 8.64 111 101 17 266 91 Medium 63 17 ## 352 10.44 124 115 16 458 105 Medium 62 16 ## 353 13.44 133 103 14 288 122 Good 61 17 ## 354 9.45 107 67 12 430 92 Medium 35 12 ## 355 5.30 133 31 1 80 145 Medium 42 18 ## 356 7.02 130 100 0 306 146 Good 42 11 ## 357 3.58 142 109 0 111 164 Good 72 12 ## 358 13.36 103 73 3 276 72 Medium 34 15 ## 359 4.17 123 96 10 71 118 Bad 69 11 ## 360 3.13 130 62 11 396 130 Bad 66 14 ## 361 8.77 118 86 7 265 114 Good 52 15 ## 362 8.68 131 25 10 183 104 Medium 56 15 ## 363 5.25 131 55 0 26 110 Bad 79 12 ## 364 10.26 111 75 1 377 108 Good 25 12 ## 365 10.50 122 21 16 488 131 Good 30 14 ## 366 6.53 154 30 0 122 162 Medium 57 17 ## 367 5.98 124 56 11 447 134 Medium 53 12 ## 368 14.37 95 106 0 256 53 Good 52 17 ## 369 10.71 109 22 10 348 79 Good 74 14 ## 370 10.26 135 100 22 463 122 Medium 36 14 ## 371 7.68 126 41 22 403 119 Bad 42 12 ## 372 9.08 152 81 0 191 126 Medium 54 16 ## 373 7.80 121 50 0 508 98 Medium 65 11 ## 374 5.58 137 71 0 402 116 Medium 78 17 ## 375 9.44 131 47 7 90 118 Medium 47 12 ## 376 7.90 132 46 4 206 124 Medium 73 11 ## 377 16.27 141 60 19 319 92 Good 44 11 ## 378 6.81 132 61 0 263 125 Medium 41 12 ## 379 6.11 133 88 3 105 119 Medium 79 12 ## 380 5.81 125 111 0 404 107 Bad 54 15 ## 381 9.64 106 64 10 17 89 Medium 68 17 ## 382 3.90 124 65 21 496 151 Bad 77 13 ## 383 4.95 121 28 19 315 121 Medium 66 14 ## 384 9.35 98 117 0 76 68 Medium 63 10 ## 385 12.85 123 37 15 348 112 Good 28 12 ## 386 5.87 131 73 13 455 132 Medium 62 17 ## 387 5.32 152 116 0 170 160 Medium 39 16 ## 388 8.67 142 73 14 238 115 Medium 73 14 ## 389 8.14 135 89 11 245 78 Bad 79 16 ## 390 8.44 128 42 8 328 107 Medium 35 12 ## 391 5.47 108 75 9 61 111 Medium 67 12 ## 392 6.10 153 63 0 49 124 Bad 56 16 ## 393 4.53 129 42 13 315 130 Bad 34 13 ## 394 5.57 109 51 10 26 120 Medium 30 17 ## 395 5.35 130 58 19 366 139 Bad 33 16 ## 396 12.57 138 108 17 203 128 Good 33 14 ## 397 6.14 139 23 3 37 120 Medium 55 11 ## 398 7.41 162 26 12 368 159 Medium 40 18 ## 399 5.94 100 79 7 284 95 Bad 50 12 ## 400 9.71 134 37 0 27 120 Good 49 16 ## Urban US ## 1 Yes Yes ## 2 Yes Yes ## 3 Yes Yes ## 4 Yes Yes ## 5 Yes No ## 6 No Yes ## 7 Yes No ## 8 Yes Yes ## 9 No No ## 10 No Yes ## 11 No Yes ## 12 Yes Yes ## 13 Yes No ## 14 Yes Yes ## 15 Yes Yes ## 16 No No ## 17 Yes No ## 18 Yes Yes ## 19 No Yes ## 20 Yes Yes ## 21 Yes Yes ## 22 No Yes ## 23 Yes No ## 24 Yes No ## 25 Yes Yes ## 26 No No ## 27 No Yes ## 28 Yes No ## 29 Yes Yes ## 30 Yes Yes ## 31 Yes No ## 32 Yes Yes ## 33 No Yes ## 34 Yes Yes ## 35 Yes Yes ## 36 No Yes ## 37 No No ## 38 Yes Yes ## 39 Yes No ## 40 No No ## 41 No No ## 42 Yes No ## 43 Yes No ## 44 Yes Yes ## 45 Yes Yes ## 46 Yes Yes ## 47 No Yes ## 48 Yes No ## 49 Yes No ## 50 Yes No ## 51 Yes Yes ## 52 Yes No ## 53 Yes Yes ## 54 Yes Yes ## 55 No Yes ## 56 Yes Yes ## 57 Yes No ## 58 Yes No ## 59 Yes Yes ## 60 Yes No ## 61 Yes Yes ## 62 No No ## 63 Yes Yes ## 64 Yes Yes ## 65 No Yes ## 66 No No ## 67 Yes No ## 68 Yes Yes ## 69 Yes Yes ## 70 Yes No ## 71 Yes Yes ## 72 No Yes ## 73 Yes No ## 74 No Yes ## 75 No Yes ## 76 No Yes ## 77 Yes Yes ## 78 No Yes ## 79 Yes Yes ## 80 Yes No ## 81 Yes Yes ## 82 Yes No ## 83 Yes Yes ## 84 Yes Yes ## 85 No No ## 86 No No ## 87 Yes No ## 88 No Yes ## 89 Yes Yes ## 90 No No ## 91 No No ## 92 Yes Yes ## 93 Yes No ## 94 Yes No ## 95 Yes Yes ## 96 Yes Yes ## 97 No Yes ## 98 Yes Yes ## 99 No Yes ## 100 No Yes ## 101 No Yes ## 102 Yes No ## 103 No No ## 104 Yes Yes ## 105 Yes No ## 106 Yes Yes ## 107 No No ## 108 Yes No ## 109 Yes No ## 110 No No ## 111 Yes Yes ## 112 Yes Yes ## 113 Yes Yes ## 114 Yes Yes ## 115 Yes Yes ## 116 Yes No ## 117 No No ## 118 Yes No ## 119 Yes Yes ## 120 Yes Yes ## 121 Yes Yes ## 122 Yes Yes ## 123 Yes Yes ## 124 No Yes ## 125 Yes No ## 126 No No ## 127 Yes Yes ## 128 Yes Yes ## 129 Yes Yes ## 130 No Yes ## 131 Yes Yes ## 132 Yes No ## 133 Yes Yes ## 134 Yes Yes ## 135 Yes No ## 136 No Yes ## 137 No No ## 138 Yes No ## 139 Yes Yes ## 140 No Yes ## 141 Yes Yes ## 142 Yes No ## 143 Yes No ## 144 Yes Yes ## 145 No No ## 146 Yes Yes ## 147 Yes No ## 148 No Yes ## 149 No Yes ## 150 Yes Yes ## 151 No Yes ## 152 No Yes ## 153 No No ## 154 No Yes ## 155 No Yes ## 156 Yes No ## 157 Yes No ## 158 No Yes ## 159 No Yes ## 160 No No ## 161 No No ## 162 No Yes ## 163 Yes No ## 164 No No ## 165 No Yes ## 166 Yes Yes ## 167 Yes Yes ## 168 Yes No ## 169 Yes No ## 170 Yes Yes ## 171 Yes Yes ## 172 Yes Yes ## 173 Yes Yes ## 174 Yes Yes ## 175 No No ## 176 Yes No ## 177 No Yes ## 178 Yes Yes ## 179 No Yes ## 180 Yes Yes ## 181 Yes Yes ## 182 Yes No ## 183 Yes No ## 184 Yes Yes ## 185 No Yes ## 186 Yes Yes ## 187 No No ## 188 Yes No ## 189 Yes No ## 190 No Yes ## 191 No Yes ## 192 Yes Yes ## 193 No No ## 194 Yes Yes ## 195 Yes Yes ## 196 Yes Yes ## 197 Yes Yes ## 198 Yes No ## 199 Yes Yes ## 200 Yes Yes ## 201 No No ## 202 Yes No ## 203 No Yes ## 204 Yes No ## 205 Yes No ## 206 Yes No ## 207 Yes Yes ## 208 No No ## 209 Yes No ## 210 No Yes ## 211 No Yes ## 212 Yes Yes ## 213 Yes Yes ## 214 Yes Yes ## 215 Yes Yes ## 216 Yes Yes ## 217 Yes No ## 218 No No ## 219 Yes Yes ## 220 Yes Yes ## 221 Yes Yes ## 222 Yes No ## 223 Yes Yes ## 224 Yes Yes ## 225 No No ## 226 Yes No ## 227 Yes No ## 228 Yes Yes ## 229 No Yes ## 230 No No ## 231 No No ## 232 No No ## 233 Yes Yes ## 234 No Yes ## 235 No Yes ## 236 Yes Yes ## 237 Yes Yes ## 238 Yes Yes ## 239 Yes No ## 240 Yes Yes ## 241 Yes No ## 242 Yes No ## 243 No No ## 244 Yes Yes ## 245 Yes No ## 246 No Yes ## 247 Yes Yes ## 248 Yes No ## 249 Yes Yes ## 250 Yes No ## 251 Yes Yes ## 252 Yes Yes ## 253 Yes No ## 254 No Yes ## 255 Yes Yes ## 256 Yes Yes ## 257 Yes No ## 258 Yes Yes ## 259 No No ## 260 No Yes ## 261 Yes Yes ## 262 Yes Yes ## 263 Yes Yes ## 264 Yes Yes ## 265 Yes Yes ## 266 Yes Yes ## 267 No Yes ## 268 No Yes ## 269 Yes No ## 270 Yes No ## 271 Yes No ## 272 Yes No ## 273 Yes No ## 274 Yes Yes ## 275 Yes Yes ## 276 Yes Yes ## 277 Yes Yes ## 278 Yes Yes ## 279 No Yes ## 280 Yes Yes ## 281 Yes Yes ## 282 No Yes ## 283 Yes No ## 284 No No ## 285 No No ## 286 Yes Yes ## 287 No Yes ## 288 Yes Yes ## 289 No No ## 290 Yes Yes ## 291 No Yes ## 292 Yes No ## 293 Yes Yes ## 294 Yes No ## 295 Yes Yes ## 296 No Yes ## 297 Yes Yes ## 298 Yes Yes ## 299 Yes No ## 300 No Yes ## 301 Yes Yes ## 302 Yes Yes ## 303 Yes Yes ## 304 Yes Yes ## 305 Yes Yes ## 306 Yes Yes ## 307 Yes Yes ## 308 Yes No ## 309 Yes Yes ## 310 Yes Yes ## 311 Yes Yes ## 312 Yes Yes ## 313 Yes Yes ## 314 Yes No ## 315 Yes Yes ## 316 Yes Yes ## 317 Yes Yes ## 318 No No ## 319 No Yes ## 320 No Yes ## 321 Yes Yes ## 322 Yes No ## 323 Yes Yes ## 324 Yes Yes ## 325 Yes Yes ## 326 Yes Yes ## 327 Yes No ## 328 Yes Yes ## 329 Yes Yes ## 330 Yes Yes ## 331 No No ## 332 Yes Yes ## 333 Yes Yes ## 334 Yes Yes ## 335 Yes Yes ## 336 Yes Yes ## 337 Yes No ## 338 Yes No ## 339 Yes No ## 340 Yes Yes ## 341 Yes No ## 342 No No ## 343 No Yes ## 344 Yes Yes ## 345 No Yes ## 346 Yes No ## 347 No No ## 348 No No ## 349 Yes Yes ## 350 No Yes ## 351 No Yes ## 352 No Yes ## 353 Yes Yes ## 354 No Yes ## 355 Yes Yes ## 356 Yes No ## 357 Yes No ## 358 Yes Yes ## 359 Yes Yes ## 360 Yes Yes ## 361 No Yes ## 362 No Yes ## 363 Yes Yes ## 364 Yes No ## 365 Yes Yes ## 366 No No ## 367 No Yes ## 368 Yes No ## 369 No Yes ## 370 Yes Yes ## 371 Yes Yes ## 372 Yes No ## 373 No No ## 374 Yes No ## 375 Yes Yes ## 376 Yes No ## 377 Yes Yes ## 378 No No ## 379 Yes Yes ## 380 Yes No ## 381 Yes Yes ## 382 Yes Yes ## 383 Yes Yes ## 384 Yes No ## 385 Yes Yes ## 386 Yes Yes ## 387 Yes No ## 388 No Yes ## 389 Yes Yes ## 390 Yes Yes ## 391 Yes Yes ## 392 Yes No ## 393 Yes Yes ## 394 No Yes ## 395 Yes Yes ## 396 Yes Yes ## 397 No Yes ## 398 Yes Yes ## 399 Yes Yes ## 400 Yes Yes # create a recipe rec_spec &lt;- recipes::recipe(Sales ~ ., data = Carseats) %&gt;% step_dummy(all_nominal()) %&gt;% step_interact(~ Income:Advertising + Price:Age) # create a workflow carseats_wf &lt;- workflows::workflow() %&gt;% add_model(lm_spec_2) %&gt;% add_recipe(rec_spec) # run the workflow wf_out &lt;- carseats_wf %&gt;% fit(Carseats) tidy(wf_out) #%&gt;% ## # A tibble: 14 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 6.58 1.01 6.52 2.22e- 10 ## 2 CompPrice 0.0929 0.00412 22.6 1.64e- 72 ## 3 Income 0.0109 0.00260 4.18 3.57e- 5 ## 4 Advertising 0.0702 0.0226 3.11 2.03e- 3 ## 5 Population 0.000159 0.000368 0.433 6.65e- 1 ## 6 Price -0.101 0.00744 -13.5 1.74e- 34 ## 7 Age -0.0579 0.0160 -3.63 3.18e- 4 ## 8 Education -0.0209 0.0196 -1.06 2.88e- 1 ## 9 ShelveLoc_Good 4.85 0.153 31.7 1.38e-109 ## 10 ShelveLoc_Medium 1.95 0.126 15.5 1.34e- 42 ## 11 Urban_Yes 0.140 0.112 1.25 2.13e- 1 ## 12 US_Yes -0.158 0.149 -1.06 2.91e- 1 ## 13 Income_x_Advertising 0.000751 0.000278 2.70 7.29e- 3 ## 14 Price_x_Age 0.000107 0.000133 0.801 4.24e- 1 #kableExtra::kable() 3.2 Conceptual questions Question 1: null hypotheses for p-values in Table 3.4 Each of the four p-values in table 3.4 relate to the null hypotheses that each corresponding coefficient is zero. Based on these p-values we can conclude that there is strong evidence that the Intercept, and the TV and Radio predictors are non zero. That is the null hypotheses corresponding to each of these coefficients can be rejected. Or in other words, the TV and Radio variables can be helpful in predicting Sales. We cannot reject the null hypothesis associated the Newspaper variable, as the corresponding p-value is greater than 1 minus the confidence level (assumed to be 95% in this case). In other words there is not strong evidence that Newspaper (in conjuction with the other predictors) can be helpful in predicting Sales. Question 2: differences between KNN classifier and KNN regression methods The key differences between the KNN classifier and KNN regression methods can be summarized as follows: For the KNN classifier the predicted value is a class (i.e. a qualitative value) whereas for the KNN regression the predicted value is numerical (i.e. a continuous numerical value). When using the classifier method, the predicted class is determined by considering the number of K nearest neighbors which fall in each class. The class with the largest number of K nearest neighbors is taken as the prediction. Whereas, when using the regression method, the predicted class is determined by averaging the values of the response variable for the K nearest neighbors. Question 3: interpreting coefficients (a): It is likely that (ii) is correct. The coefficient for the main effect gender is positive (B3 is 35), so for a fixed value of GPA and IQ we would expect females to earn more. There is an interaction term, between GPA and gender but this is not relevant because we are assuming GPA is a fixed value. (b): gender &lt;- 1 IQ &lt;- 110 GPA &lt;- 4.0 salary &lt;- 50 + (20 * GPA) + (0.07 * IQ) + (35 * gender) + (0.01 * GPA * IQ) + (-10 * GPA * gender) salary ## [1] 137.1 (c): True - given the GPA/IQ coefficient is very small it is unlikely that its p-value would show statistical significant, because assume the standard error of the coefficient is non-zero the confidence interval for the coefficient will encompass zero. Question 4: linear versus cubic regression (a): For the training set, I dont think there is enough information to be confident about the question of if the RSS would be lower for the linear or cubic regression. It would depend on if the noise in the data, on top of the underlying linear relationship, created a resemblance to a cubic relationship between the predictor and response variable. (b): For the test set, I would expect the linear regression to have a lower RSS than the cubic regression. This is because cubic regression (as the more flexible method) would have been more likely to over fit the data. (c): If we assume the underlying relationship is non linear, again I think there is not enough information to be confident about the question of if the RSS would be lower for the linear or cubic regression. It would depend on whether the assumed functional form of the linear and cubic models is closer to the true underlying relationship. (d): Again for the test, it would depend on whether the assumed functional form of the linear and cubic models is closer to the true underlying relationship. Question 5 Answer in notebook. 3.3 Applied questions "]]
