# Statistical Learning

```{r}
library(tidyverse)
```


```{r, include = FALSE}
# global settings for producing html output
knitr::opts_chunk$set(warning = FALSE,  # remove for readability
                      message = FALSE)#,  # remove for readability
                      #cache = TRUE)   # for inclusion during coding

# set default theme for plots
theme_set(theme_minimal())
```

## Conceptual Exercises

**Question 1: Flexible and inflexible statistical learning methods.**

*(a):* If sample size (n) is very large and the number of predictors is small (p) I would expect a flexible statistical learning method to perform better than an inflexible method. This is where sample sizes are large one can assume that the variance will be a lesser contributor to the irreducible error than bias. And, the bias of the more flexible method will be lower than the bias of the more flexible method.

*(b):* If there is a large number of predictors (p) and a small number of data points (n) I would expect a flexible statistical learning method to perform worse than an inflexible method. This is because a flexible method would be likely to have high variance (i.e. the component of the reducible error associated with the differences between model prediction over different training sets). Hence, the more flexible method would be more prone to overfitting the small number of training data points (than the inflexible method) and in turn be likely to return worse results. This is of course a generalisation and the performance of flexible vs. inflexible methods varies from dataset to dataset.

*(c):* If the relationship between the response variable and the predictor variables is highly non linear I would expect a flexible statistical learning method to perform better than an inflexible method. This is because a relatively inflexible methods (such as linear regression) tend to have assumptions around linearity embedded within them, whereas more flexible methods do not (e.g. splines).

*(d):* If the variance of the error terms is extremely high I would expect a flexible statistical learning method to perform worse than an inflexible method. This is because with a flexible model there would a tendency to overfit the large amount noise (i.e. very high variance in error terms) in the data. Whereas, the less flexible method would be less sensitive to the noise and in many cases could well do a better job of extracting the signal (i.e. relationship) between the predictors and the response variable.

**Question 2: Regression or classification problems.**

*(a):*

-   Problem type: regression (salary is a continuous variable);

-   n = 500;

-   and, p = 3.

*(b):*

-   Problem type: classification (success or failure is a binary variable);

-   n = 20;

-   and, p = 13.

*(c):*

-   Problem type - regression (% change is a continuous variable);

-   n = 52;

-   and, p = 3.

**Question 3: Bias-variance decomposition**

*(a):* sketched in paper notebook.

*(b):* describing the shape of typical error curves

-   Bias (squared) - reduces as the flexibility of method increases because the bias represents the errors by approximating and simplifying a real world system. Less flexible methods apply more simplifications than flexible methods and hence have greater bias.

-   Variance - increase as the flexibility of a method increases because the variance represents the error arising from sample process. More flexible methods are more closely fitted to the specific features of the data than less flexible models. So, overfitting of the current sample increases of model flexibility, driving increases in variance errors.

-   The Training error - decrease as the flexibility of a method increases (following the bias curve plus the irreducible error). This is because variance errors are not evident in fitting to the training set, rather overfitting actually reduce training errors (giving an overly optimistic perspective on the likely performance of a method on new data).

-   The test error - decreases as method flexibility increases but then increases again. This is because although the test error decrease as the bias component of the errors decreases, at some point increases in the variance component of the error drives increases in the test error. In other words there is a point at which the advantages of increase flexibility (in terms of reducing the bias component) are outweigh by the disadvantages (in terms of overfitting as the variance component increases.

-   The irreducible error - is constant and by definition is the component of the error that is unaffected by method choice and parameterisation.

**Question 4: Examples problems which are well suited to statistical learning methods.**

Classification problems:

-   Identifying if an email is or is not spam is a problem where predictions are required. The response variable would be flag indicating if an email is spam or not (in the training set this might have been set by manually reviewing the email). The predictor variables would be characteristics of the email for example the use of certain words that are prevalent in spam.

-   Explaining difference in the proportion of professors who have tenured. The response variable would be a variable indicating if the professor in question has tenure. The explanatory variables could be characteristic including the gender of the professor, if they have a minority ethnic background etc.

-   Predicting if an athlete will win a medal (or not) at the Olympics in a specific event. Predictor variables here could include for example performance at past Olympics, personal bests etc.

Regression problems:

-   Estimating the amount of plastic waste produced by a country. Here predictor variables could include GDP, population, quantitative measure of the maturity of the waste disposal system etc.

-   Predicting if a given individual will make use of green spaces in their local area. Here predictor variables might include self-reported health status, age etc.

-   Explaining the Green House Gas emission reductions (or increases) delivered by a country. Here predictor values might include GDP per capita, a categorical variable indicating how supportive a countries is at COP meetings, the political leanings of the governing party (on a left-right axis) etc.

Clustering problems:

-   Identifying if there are groups of neighborhoods within a city facing similar problems with citizens accessing green space. Here features for clustering might include the proportion of the population within easy walking distance of public green space, the amount of public green space in the neighborhood, the proportion of the population experiencing disabilities.

-   Identifying if there are groups of customer (of a given business) with similar purchasing habit. Here feature for clustering might total spend per month with the business, the frequency of purchases etc.

-   Identify if groups of people with similar attitudes to car sharing can be identified from survey responses. Here features for clustering might include Likhert scales responses asking about attitudes to car sharing.

**Question 5: advantages and disadvantages of a very flexible method (relative to an inflexible method).**

*Advantages:*

-   ability to capture non-linear relationships between variables;

-   ability to pick up relatively subtle associations between variables;

-   can deliver more accurate prediction by modeling more of the complexity of real world systems.

*Disadvantages:*

-   prone to overfitting;

-   large numbers of data points are required;

-   computational intensive;

-   can be difficult to interpret the model, and hence not so useful for inference.

**Question 6: parametric and non-parametric methods**

A parametric model makes as assumptions about the functional form (i.e. shape) of *f.* Where *f* is the function mapping the values of predictor/s to the values of the response variable. Based on these assumptions the general form of the model can be mathematical expressed including the predictor variables and associated co-efficients. Once a parametric model has been selected it can be fit to the data, this reduces to a problem of estimating the coefficients in the mathematical model. The primary advantage of this approach is its simplicity (estimating a set of parameters is much simpler than estimating f itself). The disadvantages are that: it can be difficult to know which model (i.e. shape) to select and picking the wrong model results in poor results; and, that flexible models are prone to overfitting.

A non-parametric model does not make any assumptions about the functional form of *f.* Rather such models attempt to estimate *f* by fitting the points as closely as possible, subject to specified constraints on how 'wiggly' or smooth a line should result. The main advantage of non-parametric models is that they can fit a wide variety of shapes for *f* and the analyst does not need to make any *a priori* guesses about what the associated between predictor and response variables are. However, this advantage comes at the cost of complexity. As the problems of estimating *f* is not reduced to one of estimating parameters, very large numbers of data points are needed. Also, given non-parametric models are not constrained by assumption about the shape of *f* then care is needed to avoid overfitting (i.e. an appropriate smoothness constraint must be defined by the analyst).

**Question 7: K-nearest neighbors**

*(a):* the euclidean distance between each data point and a test point (x~1~ = x~2~ = x~3~ = 0) is shown below.

```{r}
library(tidyverse)

data <- read_csv("2_7.csv") %>% 
  mutate(dist_from_test_point = sqrt(x1^2 + x2^2 + x3^2))

data %>% 
  knitr::kable()

data %>% 
  select(obs, dist_from_test_point) %>% 
  knitr::kable()

```

*(b):* Using the k-nearest neighbor method with k = 1 for the test point (x~1~ = x~2~ = x~3~ = 0) the prediction would be the Y value of the nearest point. The nearest point is 5 so the prediction would be `Green`.

*(c):* If k =3, k-nearest neighbor would look at the three nearest points (y~2~ = `Red`, y~5~ = `Green` and y~6~ = `Red`). The predicted y value would be `Red` as the majority of the k nearest points have an observed y values of `Red`.

## Applied Exercises

**Question 8: The Colleges Dataset**

*(a):* first I read in the data for the package.

```{r}
library(ISLR)
library(pander)
college <- as_tibble(College)
```

*(b):* There college names are already stored as row names

```{r}
pander(head(college))
```

*(c):* First I produce the summary statistics.

```{r}
college %>% 
  skimr::skim()
```

Then produce a scatter plot matrix for the first 10 variables.

```{r echo=FALSE}
GGally::ggpairs(college[,1:10], progress = FALSE)
```

*(c):* next is to plot side by side boxplots of `outstate` vs `private`

```{r}
college %>% 
  ggplot(aes(x = Outstate, y = Private)) +
  geom_boxplot() +
  coord_flip() +
  
  labs(x = "Private university?",
       y = "Number of out of state students",
       title = "Private universities tend to have more out of state students")
```

*(iv):* next is looking at universities with high proportions of students from elite high schools.

```{r}
college %>% 
  mutate(elite = if_else(Top10perc > 50,
                         TRUE,
                         FALSE)) %>% 
  group_by(elite) %>% 
  summarise(n = n())
  
```
*(v):* next looking at producing some histograms for a couple of continuous variables of interest (full and part time undergraduates)
```{r}
library(patchwork)

p <- college %>% 
  select(F.Undergrad, P.Undergrad) %>% 
  mutate(id = row_number(), .before = F.Undergrad) %>% 
  pivot_longer(cols = c(F.Undergrad, P.Undergrad), 
               names_to = "student_status", 
               values_to = "number_of_students") %>% 
  
  ggplot(aes(number_of_students)) +
  geom_histogram(aes(fill = student_status), bins = 100) +
  facet_wrap(~student_status) +
  guides(fill = FALSE) +
  labs(x = "Number of students attending",
       y = "Number of universities")

p_zoom <- p + coord_cartesian(xlim = c(0,2500)) +
  labs(subtitle = "Zooming on smaller cohort sizes")

p / p_zoom +
  plot_annotation(title = "Differing frequencies of full and part-time students cohort sizes")
```


